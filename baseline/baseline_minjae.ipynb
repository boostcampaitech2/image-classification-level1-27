{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#import \n",
    "import os\n",
    "import sys\n",
    "import gzip\n",
    "import random\n",
    "import platform\n",
    "import warnings\n",
    "import collections\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import re\n",
    "from sklearn.metrics import f1_score\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.datasets import load_iris\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler, WeightedRandomSampler\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Random Seed Set\n",
    "SEED = 17\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)  # type: ignore\n",
    "torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "torch.backends.cudnn.benchmark = True  # type: ignore"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# 현재 OS 및 라이브러리 버전 체크 체크\n",
    "current_os = platform.system()\n",
    "print(f\"Current OS: {current_os}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "print(f\"Python Version: {platform.python_version()}\")\n",
    "print(f\"torch Version: {torch.__version__}\")\n",
    "print(f\"torchvision Version: {torchvision.__version__}\")\n",
    "\n",
    "# 중요하지 않은 에러 무시\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "# 유니코드 깨짐현상 해결\n",
    "mpl.rcParams['axes.unicode_minus'] = False"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Current OS: Linux\n",
      "CUDA: True\n",
      "Python Version: 3.8.5\n",
      "torch Version: 1.7.1\n",
      "torchvision Version: 0.8.2\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "test_dir = '/opt/ml/input/data/eval'\n",
    "train_dir = '/opt/ml/input/data/train'\n",
    "train_img_dir = train_dir+'/images'\n",
    "test_img_dir = test_dir + \"/images\"\n",
    "data_info = train_dir + '/train.csv'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "data = pd.read_csv(train_dir+\"/train.csv\")\n",
    "list_path = data['path'].to_list()\n",
    "condition_list = [\n",
    "    (data['age']<30),\n",
    "    (30<=data['age'])&(data['age']<60),\n",
    "    (data['age']>=60)\n",
    "]\n",
    "condition_names = [\"lower30\",\"30to60\",\"upper60\"]\n",
    "data['age_cate'] = np.select(condition_list, condition_names)\n",
    "dict_label = {path : data[data['path']==path][['gender', 'age_cate']].values.tolist()[0] for path in list_path}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def read_images(path, labels = None, mode= None):\n",
    "    if labels:\n",
    "        train_x, train_y = [], []\n",
    "\n",
    "        if mode == \"mask\":\n",
    "            for label in labels:\n",
    "                label_dir = path+\"/\"+label\n",
    "                file_list = [x for x in os.listdir(label_dir) if x[0] != \".\"]\n",
    "                for file_name in  file_list:\n",
    "                    img = Image.open(label_dir+\"/\"+file_name)\n",
    "                    train_x.append(img)\n",
    "                    if \"incorrect\" in file_name :\n",
    "                        train_y.append(2)\n",
    "                    elif \"normal\" in file_name:\n",
    "                        train_y.append(1)\n",
    "                    elif \"mask\" in file_name:\n",
    "                        train_y.append(0)\n",
    "            return train_x, train_y\n",
    "\n",
    "        elif mode == \"age_cate\":\n",
    "            for label in labels:\n",
    "                label_dir = path+\"/\"+label\n",
    "                file_list = [x for x in os.listdir(label_dir) if x[0] != \".\"]\n",
    "                for file_name in  file_list:\n",
    "                    img = Image.open(label_dir+\"/\"+file_name)\n",
    "                    train_x.append(img)\n",
    "                    age_cate = dict_label[label][1]\n",
    "                    if age_cate == \"upper60\":\n",
    "                        train_y.append(2)\n",
    "                    elif age_cate == \"30to60\":\n",
    "                        train_y.append(1)\n",
    "                    elif age_cate == \"lower30\":\n",
    "                        train_y.append(0)\n",
    "            return train_x, train_y\n",
    "            \n",
    "        elif mode == \"gender\":\n",
    "            for label in labels:\n",
    "                label_dir = path+\"/\"+label\n",
    "                file_list = [x for x in os.listdir(label_dir) if x[0] != \".\"]\n",
    "                for file_name in  file_list:\n",
    "                    img = Image.open(label_dir+\"/\"+file_name)\n",
    "                    train_x.append(img)\n",
    "                    gender = dict_label[label][0]\n",
    "                    if gender == \"female\":\n",
    "                        train_y.append(1)\n",
    "                    elif gender == \"male\":\n",
    "                        train_y.append(0)\n",
    "            return train_x, train_y\n",
    " \n",
    "    else:\n",
    "        train_x = []\n",
    "        id_list = []\n",
    "        file_list = [x for x in os.listdir(path) if x[0] != \".\"]\n",
    "        for file_name in  file_list:\n",
    "            id_list.append(file_name)\n",
    "            img = Image.open(path+\"/\"+file_name)\n",
    "            train_x.append(img)\n",
    "        return train_x, id_list\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=0.5, std=0.5, inplace=True),\n",
    "                                transforms.Resize((128,128))])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "class MASKDataset(Dataset):\n",
    "    def __init__(self, path, transform=None, train=True, labels=None, mode = None):\n",
    "        if train :\n",
    "            self.mode = mode\n",
    "            self.X, self.y = read_images(path, labels,mode)\n",
    "        else:\n",
    "            self.X, self.id_list = read_images(path)\n",
    "        \n",
    "        self.train = train\n",
    "        self._repr_indent = 4\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "        self.classes = ['Wear', 'NotWear', 'Incorrect']\n",
    " \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def get_id_list(self):\n",
    "        return self.id_list\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if self.train :\n",
    "            X,y = self.X[idx], self.y[idx]\n",
    "        else:\n",
    "            X = self.X[idx]\n",
    "        if self.transform:\n",
    "            X = self.transform(X)\n",
    "        if self.train :\n",
    "            return X, y\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        '''\n",
    "        https://github.com/pytorch/vision/blob/master/torchvision/datasets/vision.py\n",
    "        '''\n",
    "        head = \"MASK Dataset\\n\"\n",
    "        data_path = self._repr_indent*\" \" + \"Data path: {}\".format(self.path)\n",
    "        num_data = self._repr_indent*\" \" + \"Number of datapoints: {}\".format(self.__len__())\n",
    "        num_classes = self._repr_indent*\" \" + \"Number of classes: {}\".format(len(self.classes))\n",
    "\n",
    "        return '\\n'.join([head,\n",
    "                          data_path, \n",
    "                          num_data, num_classes])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "test_idx = pd.Series(pd.Series(list_path).index).sample(frac=0.2, random_state=SEED).to_list()\n",
    "test_labels = [list_path[idx] for idx in test_idx]\n",
    "train_labels = pd.Series(list_path)\n",
    "train_labels = train_labels.drop(test_idx).to_list()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "len(train_labels), len(test_labels)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(2160, 540)"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "source": [
    "m_train_set = MASKDataset(path=train_img_dir, train=True, labels=train_labels, transform=transform, mode=\"mask\")\n",
    "m_validation_set = MASKDataset(path=train_img_dir, train=True, labels=test_labels, transform=transform, mode=\"mask\")\n",
    "m_total_set = MASKDataset(path=train_img_dir, train=True, labels=list_path, transform=transform, mode=\"mask\")\n",
    "\n",
    "g_train_set = MASKDataset(path=train_img_dir, train=True, labels=train_labels, transform=transform, mode=\"gender\")\n",
    "g_validation_set = MASKDataset(path=train_img_dir, train=True, labels=test_labels, transform=transform, mode=\"gender\")\n",
    "g_total_set = MASKDataset(path=train_img_dir, train=True, labels=list_path, transform=transform, mode=\"gender\")\n",
    "\n",
    "a_train_set = MASKDataset(path=train_img_dir, train=True, labels=train_labels, transform=transform, mode=\"age_cate\")\n",
    "a_validation_set = MASKDataset(path=train_img_dir, train=True, labels=test_labels, transform=transform, mode=\"age_cate\")\n",
    "a_total_set = MASKDataset(path=train_img_dir, train=True, labels=list_path, transform=transform, mode=\"age_cate\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "eval_set = MASKDataset(path=test_img_dir, train=False, transform=transform)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "len(g_train_set), len(g_validation_set), len(g_total_set), len(eval_set)\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(15120, 3780, 18900, 12600)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "m_train_loader = DataLoader(dataset=m_train_set, batch_size=16, shuffle=True,num_workers=0)\n",
    "m_valid_loader = DataLoader(dataset=m_validation_set, batch_size=16, shuffle=True, num_workers=0)\n",
    "m_total_loader = DataLoader(dataset=m_total_set, batch_size=16, shuffle=True, num_workers=0)\n",
    "\n",
    "g_train_loader = DataLoader(dataset=g_train_set, batch_size=16, shuffle=True,num_workers=0)\n",
    "g_valid_loader = DataLoader(dataset=g_validation_set, batch_size=16, shuffle=True, num_workers=0)\n",
    "g_total_loader = DataLoader(dataset=g_total_set, batch_size=16, shuffle=True, num_workers=0)\n",
    "\n",
    "a_train_loader = DataLoader(dataset=a_train_set, batch_size=16, shuffle=True,num_workers=0)\n",
    "a_valid_loader = DataLoader(dataset=a_validation_set, batch_size=16, shuffle=True, num_workers=0)\n",
    "a_total_loader = DataLoader(dataset=a_total_set, batch_size=16, shuffle=True, num_workers=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "eval_loader = DataLoader(dataset=eval_set, batch_size=1, shuffle=False, num_workers=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "import torchvision.models.resnet as resnet\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "conv1x1=resnet.conv1x1\n",
    "Bottleneck = resnet.Bottleneck\n",
    "BasicBlock= resnet.BasicBlock\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=3, zero_init_residual=True):\n",
    "        super().__init__()\n",
    "        self.inplanes = 32\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 32, layers[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 64, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 128, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 256, layers[3], stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Linear(256* block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight,0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1 ):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion :\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes*block.expansion, stride),\n",
    "                nn.BatchNorm2d(planes * block.expansion)\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x= self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "resnet50 = ResNet(resnet.Bottleneck, [3, 4, 6, 3], 3, True).to(device)\n",
    "resnet50_gender = ResNet(resnet.Bottleneck, [3, 4, 6, 3], 2, True).to(device)\n",
    "# resnet50"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "from torchsummary import summary\n",
    "summary(resnet50, input_size=(3,128,128), device=device)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 128, 128]             864\n",
      "       BatchNorm2d-2         [-1, 32, 128, 128]              64\n",
      "              ReLU-3         [-1, 32, 128, 128]               0\n",
      "         MaxPool2d-4           [-1, 32, 64, 64]               0\n",
      "            Conv2d-5           [-1, 32, 64, 64]           1,024\n",
      "       BatchNorm2d-6           [-1, 32, 64, 64]              64\n",
      "              ReLU-7           [-1, 32, 64, 64]               0\n",
      "            Conv2d-8           [-1, 32, 64, 64]           9,216\n",
      "       BatchNorm2d-9           [-1, 32, 64, 64]              64\n",
      "             ReLU-10           [-1, 32, 64, 64]               0\n",
      "           Conv2d-11          [-1, 128, 64, 64]           4,096\n",
      "      BatchNorm2d-12          [-1, 128, 64, 64]             256\n",
      "           Conv2d-13          [-1, 128, 64, 64]           4,096\n",
      "      BatchNorm2d-14          [-1, 128, 64, 64]             256\n",
      "             ReLU-15          [-1, 128, 64, 64]               0\n",
      "       Bottleneck-16          [-1, 128, 64, 64]               0\n",
      "           Conv2d-17           [-1, 32, 64, 64]           4,096\n",
      "      BatchNorm2d-18           [-1, 32, 64, 64]              64\n",
      "             ReLU-19           [-1, 32, 64, 64]               0\n",
      "           Conv2d-20           [-1, 32, 64, 64]           9,216\n",
      "      BatchNorm2d-21           [-1, 32, 64, 64]              64\n",
      "             ReLU-22           [-1, 32, 64, 64]               0\n",
      "           Conv2d-23          [-1, 128, 64, 64]           4,096\n",
      "      BatchNorm2d-24          [-1, 128, 64, 64]             256\n",
      "             ReLU-25          [-1, 128, 64, 64]               0\n",
      "       Bottleneck-26          [-1, 128, 64, 64]               0\n",
      "           Conv2d-27           [-1, 32, 64, 64]           4,096\n",
      "      BatchNorm2d-28           [-1, 32, 64, 64]              64\n",
      "             ReLU-29           [-1, 32, 64, 64]               0\n",
      "           Conv2d-30           [-1, 32, 64, 64]           9,216\n",
      "      BatchNorm2d-31           [-1, 32, 64, 64]              64\n",
      "             ReLU-32           [-1, 32, 64, 64]               0\n",
      "           Conv2d-33          [-1, 128, 64, 64]           4,096\n",
      "      BatchNorm2d-34          [-1, 128, 64, 64]             256\n",
      "             ReLU-35          [-1, 128, 64, 64]               0\n",
      "       Bottleneck-36          [-1, 128, 64, 64]               0\n",
      "           Conv2d-37           [-1, 64, 64, 64]           8,192\n",
      "      BatchNorm2d-38           [-1, 64, 64, 64]             128\n",
      "             ReLU-39           [-1, 64, 64, 64]               0\n",
      "           Conv2d-40           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-41           [-1, 64, 32, 32]             128\n",
      "             ReLU-42           [-1, 64, 32, 32]               0\n",
      "           Conv2d-43          [-1, 256, 32, 32]          16,384\n",
      "      BatchNorm2d-44          [-1, 256, 32, 32]             512\n",
      "           Conv2d-45          [-1, 256, 32, 32]          32,768\n",
      "      BatchNorm2d-46          [-1, 256, 32, 32]             512\n",
      "             ReLU-47          [-1, 256, 32, 32]               0\n",
      "       Bottleneck-48          [-1, 256, 32, 32]               0\n",
      "           Conv2d-49           [-1, 64, 32, 32]          16,384\n",
      "      BatchNorm2d-50           [-1, 64, 32, 32]             128\n",
      "             ReLU-51           [-1, 64, 32, 32]               0\n",
      "           Conv2d-52           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-53           [-1, 64, 32, 32]             128\n",
      "             ReLU-54           [-1, 64, 32, 32]               0\n",
      "           Conv2d-55          [-1, 256, 32, 32]          16,384\n",
      "      BatchNorm2d-56          [-1, 256, 32, 32]             512\n",
      "             ReLU-57          [-1, 256, 32, 32]               0\n",
      "       Bottleneck-58          [-1, 256, 32, 32]               0\n",
      "           Conv2d-59           [-1, 64, 32, 32]          16,384\n",
      "      BatchNorm2d-60           [-1, 64, 32, 32]             128\n",
      "             ReLU-61           [-1, 64, 32, 32]               0\n",
      "           Conv2d-62           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-63           [-1, 64, 32, 32]             128\n",
      "             ReLU-64           [-1, 64, 32, 32]               0\n",
      "           Conv2d-65          [-1, 256, 32, 32]          16,384\n",
      "      BatchNorm2d-66          [-1, 256, 32, 32]             512\n",
      "             ReLU-67          [-1, 256, 32, 32]               0\n",
      "       Bottleneck-68          [-1, 256, 32, 32]               0\n",
      "           Conv2d-69           [-1, 64, 32, 32]          16,384\n",
      "      BatchNorm2d-70           [-1, 64, 32, 32]             128\n",
      "             ReLU-71           [-1, 64, 32, 32]               0\n",
      "           Conv2d-72           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-73           [-1, 64, 32, 32]             128\n",
      "             ReLU-74           [-1, 64, 32, 32]               0\n",
      "           Conv2d-75          [-1, 256, 32, 32]          16,384\n",
      "      BatchNorm2d-76          [-1, 256, 32, 32]             512\n",
      "             ReLU-77          [-1, 256, 32, 32]               0\n",
      "       Bottleneck-78          [-1, 256, 32, 32]               0\n",
      "           Conv2d-79          [-1, 128, 32, 32]          32,768\n",
      "      BatchNorm2d-80          [-1, 128, 32, 32]             256\n",
      "             ReLU-81          [-1, 128, 32, 32]               0\n",
      "           Conv2d-82          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-83          [-1, 128, 16, 16]             256\n",
      "             ReLU-84          [-1, 128, 16, 16]               0\n",
      "           Conv2d-85          [-1, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-86          [-1, 512, 16, 16]           1,024\n",
      "           Conv2d-87          [-1, 512, 16, 16]         131,072\n",
      "      BatchNorm2d-88          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-89          [-1, 512, 16, 16]               0\n",
      "       Bottleneck-90          [-1, 512, 16, 16]               0\n",
      "           Conv2d-91          [-1, 128, 16, 16]          65,536\n",
      "      BatchNorm2d-92          [-1, 128, 16, 16]             256\n",
      "             ReLU-93          [-1, 128, 16, 16]               0\n",
      "           Conv2d-94          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-95          [-1, 128, 16, 16]             256\n",
      "             ReLU-96          [-1, 128, 16, 16]               0\n",
      "           Conv2d-97          [-1, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-98          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-99          [-1, 512, 16, 16]               0\n",
      "      Bottleneck-100          [-1, 512, 16, 16]               0\n",
      "          Conv2d-101          [-1, 128, 16, 16]          65,536\n",
      "     BatchNorm2d-102          [-1, 128, 16, 16]             256\n",
      "            ReLU-103          [-1, 128, 16, 16]               0\n",
      "          Conv2d-104          [-1, 128, 16, 16]         147,456\n",
      "     BatchNorm2d-105          [-1, 128, 16, 16]             256\n",
      "            ReLU-106          [-1, 128, 16, 16]               0\n",
      "          Conv2d-107          [-1, 512, 16, 16]          65,536\n",
      "     BatchNorm2d-108          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-109          [-1, 512, 16, 16]               0\n",
      "      Bottleneck-110          [-1, 512, 16, 16]               0\n",
      "          Conv2d-111          [-1, 128, 16, 16]          65,536\n",
      "     BatchNorm2d-112          [-1, 128, 16, 16]             256\n",
      "            ReLU-113          [-1, 128, 16, 16]               0\n",
      "          Conv2d-114          [-1, 128, 16, 16]         147,456\n",
      "     BatchNorm2d-115          [-1, 128, 16, 16]             256\n",
      "            ReLU-116          [-1, 128, 16, 16]               0\n",
      "          Conv2d-117          [-1, 512, 16, 16]          65,536\n",
      "     BatchNorm2d-118          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-119          [-1, 512, 16, 16]               0\n",
      "      Bottleneck-120          [-1, 512, 16, 16]               0\n",
      "          Conv2d-121          [-1, 128, 16, 16]          65,536\n",
      "     BatchNorm2d-122          [-1, 128, 16, 16]             256\n",
      "            ReLU-123          [-1, 128, 16, 16]               0\n",
      "          Conv2d-124          [-1, 128, 16, 16]         147,456\n",
      "     BatchNorm2d-125          [-1, 128, 16, 16]             256\n",
      "            ReLU-126          [-1, 128, 16, 16]               0\n",
      "          Conv2d-127          [-1, 512, 16, 16]          65,536\n",
      "     BatchNorm2d-128          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-129          [-1, 512, 16, 16]               0\n",
      "      Bottleneck-130          [-1, 512, 16, 16]               0\n",
      "          Conv2d-131          [-1, 128, 16, 16]          65,536\n",
      "     BatchNorm2d-132          [-1, 128, 16, 16]             256\n",
      "            ReLU-133          [-1, 128, 16, 16]               0\n",
      "          Conv2d-134          [-1, 128, 16, 16]         147,456\n",
      "     BatchNorm2d-135          [-1, 128, 16, 16]             256\n",
      "            ReLU-136          [-1, 128, 16, 16]               0\n",
      "          Conv2d-137          [-1, 512, 16, 16]          65,536\n",
      "     BatchNorm2d-138          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-139          [-1, 512, 16, 16]               0\n",
      "      Bottleneck-140          [-1, 512, 16, 16]               0\n",
      "          Conv2d-141          [-1, 256, 16, 16]         131,072\n",
      "     BatchNorm2d-142          [-1, 256, 16, 16]             512\n",
      "            ReLU-143          [-1, 256, 16, 16]               0\n",
      "          Conv2d-144            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-145            [-1, 256, 8, 8]             512\n",
      "            ReLU-146            [-1, 256, 8, 8]               0\n",
      "          Conv2d-147           [-1, 1024, 8, 8]         262,144\n",
      "     BatchNorm2d-148           [-1, 1024, 8, 8]           2,048\n",
      "          Conv2d-149           [-1, 1024, 8, 8]         524,288\n",
      "     BatchNorm2d-150           [-1, 1024, 8, 8]           2,048\n",
      "            ReLU-151           [-1, 1024, 8, 8]               0\n",
      "      Bottleneck-152           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-153            [-1, 256, 8, 8]         262,144\n",
      "     BatchNorm2d-154            [-1, 256, 8, 8]             512\n",
      "            ReLU-155            [-1, 256, 8, 8]               0\n",
      "          Conv2d-156            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-157            [-1, 256, 8, 8]             512\n",
      "            ReLU-158            [-1, 256, 8, 8]               0\n",
      "          Conv2d-159           [-1, 1024, 8, 8]         262,144\n",
      "     BatchNorm2d-160           [-1, 1024, 8, 8]           2,048\n",
      "            ReLU-161           [-1, 1024, 8, 8]               0\n",
      "      Bottleneck-162           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-163            [-1, 256, 8, 8]         262,144\n",
      "     BatchNorm2d-164            [-1, 256, 8, 8]             512\n",
      "            ReLU-165            [-1, 256, 8, 8]               0\n",
      "          Conv2d-166            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-167            [-1, 256, 8, 8]             512\n",
      "            ReLU-168            [-1, 256, 8, 8]               0\n",
      "          Conv2d-169           [-1, 1024, 8, 8]         262,144\n",
      "     BatchNorm2d-170           [-1, 1024, 8, 8]           2,048\n",
      "            ReLU-171           [-1, 1024, 8, 8]               0\n",
      "      Bottleneck-172           [-1, 1024, 8, 8]               0\n",
      "AdaptiveAvgPool2d-173           [-1, 1024, 1, 1]               0\n",
      "          Linear-174                    [-1, 3]           3,075\n",
      "================================================================\n",
      "Total params: 5,891,875\n",
      "Trainable params: 5,891,875\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 187.13\n",
      "Params size (MB): 22.48\n",
      "Estimated Total Size (MB): 209.80\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "summary(resnet50_gender, input_size=(3,128,128), device=device)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 128, 128]             864\n",
      "       BatchNorm2d-2         [-1, 32, 128, 128]              64\n",
      "              ReLU-3         [-1, 32, 128, 128]               0\n",
      "         MaxPool2d-4           [-1, 32, 64, 64]               0\n",
      "            Conv2d-5           [-1, 32, 64, 64]           1,024\n",
      "       BatchNorm2d-6           [-1, 32, 64, 64]              64\n",
      "              ReLU-7           [-1, 32, 64, 64]               0\n",
      "            Conv2d-8           [-1, 32, 64, 64]           9,216\n",
      "       BatchNorm2d-9           [-1, 32, 64, 64]              64\n",
      "             ReLU-10           [-1, 32, 64, 64]               0\n",
      "           Conv2d-11          [-1, 128, 64, 64]           4,096\n",
      "      BatchNorm2d-12          [-1, 128, 64, 64]             256\n",
      "           Conv2d-13          [-1, 128, 64, 64]           4,096\n",
      "      BatchNorm2d-14          [-1, 128, 64, 64]             256\n",
      "             ReLU-15          [-1, 128, 64, 64]               0\n",
      "       Bottleneck-16          [-1, 128, 64, 64]               0\n",
      "           Conv2d-17           [-1, 32, 64, 64]           4,096\n",
      "      BatchNorm2d-18           [-1, 32, 64, 64]              64\n",
      "             ReLU-19           [-1, 32, 64, 64]               0\n",
      "           Conv2d-20           [-1, 32, 64, 64]           9,216\n",
      "      BatchNorm2d-21           [-1, 32, 64, 64]              64\n",
      "             ReLU-22           [-1, 32, 64, 64]               0\n",
      "           Conv2d-23          [-1, 128, 64, 64]           4,096\n",
      "      BatchNorm2d-24          [-1, 128, 64, 64]             256\n",
      "             ReLU-25          [-1, 128, 64, 64]               0\n",
      "       Bottleneck-26          [-1, 128, 64, 64]               0\n",
      "           Conv2d-27           [-1, 32, 64, 64]           4,096\n",
      "      BatchNorm2d-28           [-1, 32, 64, 64]              64\n",
      "             ReLU-29           [-1, 32, 64, 64]               0\n",
      "           Conv2d-30           [-1, 32, 64, 64]           9,216\n",
      "      BatchNorm2d-31           [-1, 32, 64, 64]              64\n",
      "             ReLU-32           [-1, 32, 64, 64]               0\n",
      "           Conv2d-33          [-1, 128, 64, 64]           4,096\n",
      "      BatchNorm2d-34          [-1, 128, 64, 64]             256\n",
      "             ReLU-35          [-1, 128, 64, 64]               0\n",
      "       Bottleneck-36          [-1, 128, 64, 64]               0\n",
      "           Conv2d-37           [-1, 64, 64, 64]           8,192\n",
      "      BatchNorm2d-38           [-1, 64, 64, 64]             128\n",
      "             ReLU-39           [-1, 64, 64, 64]               0\n",
      "           Conv2d-40           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-41           [-1, 64, 32, 32]             128\n",
      "             ReLU-42           [-1, 64, 32, 32]               0\n",
      "           Conv2d-43          [-1, 256, 32, 32]          16,384\n",
      "      BatchNorm2d-44          [-1, 256, 32, 32]             512\n",
      "           Conv2d-45          [-1, 256, 32, 32]          32,768\n",
      "      BatchNorm2d-46          [-1, 256, 32, 32]             512\n",
      "             ReLU-47          [-1, 256, 32, 32]               0\n",
      "       Bottleneck-48          [-1, 256, 32, 32]               0\n",
      "           Conv2d-49           [-1, 64, 32, 32]          16,384\n",
      "      BatchNorm2d-50           [-1, 64, 32, 32]             128\n",
      "             ReLU-51           [-1, 64, 32, 32]               0\n",
      "           Conv2d-52           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-53           [-1, 64, 32, 32]             128\n",
      "             ReLU-54           [-1, 64, 32, 32]               0\n",
      "           Conv2d-55          [-1, 256, 32, 32]          16,384\n",
      "      BatchNorm2d-56          [-1, 256, 32, 32]             512\n",
      "             ReLU-57          [-1, 256, 32, 32]               0\n",
      "       Bottleneck-58          [-1, 256, 32, 32]               0\n",
      "           Conv2d-59           [-1, 64, 32, 32]          16,384\n",
      "      BatchNorm2d-60           [-1, 64, 32, 32]             128\n",
      "             ReLU-61           [-1, 64, 32, 32]               0\n",
      "           Conv2d-62           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-63           [-1, 64, 32, 32]             128\n",
      "             ReLU-64           [-1, 64, 32, 32]               0\n",
      "           Conv2d-65          [-1, 256, 32, 32]          16,384\n",
      "      BatchNorm2d-66          [-1, 256, 32, 32]             512\n",
      "             ReLU-67          [-1, 256, 32, 32]               0\n",
      "       Bottleneck-68          [-1, 256, 32, 32]               0\n",
      "           Conv2d-69           [-1, 64, 32, 32]          16,384\n",
      "      BatchNorm2d-70           [-1, 64, 32, 32]             128\n",
      "             ReLU-71           [-1, 64, 32, 32]               0\n",
      "           Conv2d-72           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-73           [-1, 64, 32, 32]             128\n",
      "             ReLU-74           [-1, 64, 32, 32]               0\n",
      "           Conv2d-75          [-1, 256, 32, 32]          16,384\n",
      "      BatchNorm2d-76          [-1, 256, 32, 32]             512\n",
      "             ReLU-77          [-1, 256, 32, 32]               0\n",
      "       Bottleneck-78          [-1, 256, 32, 32]               0\n",
      "           Conv2d-79          [-1, 128, 32, 32]          32,768\n",
      "      BatchNorm2d-80          [-1, 128, 32, 32]             256\n",
      "             ReLU-81          [-1, 128, 32, 32]               0\n",
      "           Conv2d-82          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-83          [-1, 128, 16, 16]             256\n",
      "             ReLU-84          [-1, 128, 16, 16]               0\n",
      "           Conv2d-85          [-1, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-86          [-1, 512, 16, 16]           1,024\n",
      "           Conv2d-87          [-1, 512, 16, 16]         131,072\n",
      "      BatchNorm2d-88          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-89          [-1, 512, 16, 16]               0\n",
      "       Bottleneck-90          [-1, 512, 16, 16]               0\n",
      "           Conv2d-91          [-1, 128, 16, 16]          65,536\n",
      "      BatchNorm2d-92          [-1, 128, 16, 16]             256\n",
      "             ReLU-93          [-1, 128, 16, 16]               0\n",
      "           Conv2d-94          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-95          [-1, 128, 16, 16]             256\n",
      "             ReLU-96          [-1, 128, 16, 16]               0\n",
      "           Conv2d-97          [-1, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-98          [-1, 512, 16, 16]           1,024\n",
      "             ReLU-99          [-1, 512, 16, 16]               0\n",
      "      Bottleneck-100          [-1, 512, 16, 16]               0\n",
      "          Conv2d-101          [-1, 128, 16, 16]          65,536\n",
      "     BatchNorm2d-102          [-1, 128, 16, 16]             256\n",
      "            ReLU-103          [-1, 128, 16, 16]               0\n",
      "          Conv2d-104          [-1, 128, 16, 16]         147,456\n",
      "     BatchNorm2d-105          [-1, 128, 16, 16]             256\n",
      "            ReLU-106          [-1, 128, 16, 16]               0\n",
      "          Conv2d-107          [-1, 512, 16, 16]          65,536\n",
      "     BatchNorm2d-108          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-109          [-1, 512, 16, 16]               0\n",
      "      Bottleneck-110          [-1, 512, 16, 16]               0\n",
      "          Conv2d-111          [-1, 128, 16, 16]          65,536\n",
      "     BatchNorm2d-112          [-1, 128, 16, 16]             256\n",
      "            ReLU-113          [-1, 128, 16, 16]               0\n",
      "          Conv2d-114          [-1, 128, 16, 16]         147,456\n",
      "     BatchNorm2d-115          [-1, 128, 16, 16]             256\n",
      "            ReLU-116          [-1, 128, 16, 16]               0\n",
      "          Conv2d-117          [-1, 512, 16, 16]          65,536\n",
      "     BatchNorm2d-118          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-119          [-1, 512, 16, 16]               0\n",
      "      Bottleneck-120          [-1, 512, 16, 16]               0\n",
      "          Conv2d-121          [-1, 128, 16, 16]          65,536\n",
      "     BatchNorm2d-122          [-1, 128, 16, 16]             256\n",
      "            ReLU-123          [-1, 128, 16, 16]               0\n",
      "          Conv2d-124          [-1, 128, 16, 16]         147,456\n",
      "     BatchNorm2d-125          [-1, 128, 16, 16]             256\n",
      "            ReLU-126          [-1, 128, 16, 16]               0\n",
      "          Conv2d-127          [-1, 512, 16, 16]          65,536\n",
      "     BatchNorm2d-128          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-129          [-1, 512, 16, 16]               0\n",
      "      Bottleneck-130          [-1, 512, 16, 16]               0\n",
      "          Conv2d-131          [-1, 128, 16, 16]          65,536\n",
      "     BatchNorm2d-132          [-1, 128, 16, 16]             256\n",
      "            ReLU-133          [-1, 128, 16, 16]               0\n",
      "          Conv2d-134          [-1, 128, 16, 16]         147,456\n",
      "     BatchNorm2d-135          [-1, 128, 16, 16]             256\n",
      "            ReLU-136          [-1, 128, 16, 16]               0\n",
      "          Conv2d-137          [-1, 512, 16, 16]          65,536\n",
      "     BatchNorm2d-138          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-139          [-1, 512, 16, 16]               0\n",
      "      Bottleneck-140          [-1, 512, 16, 16]               0\n",
      "          Conv2d-141          [-1, 256, 16, 16]         131,072\n",
      "     BatchNorm2d-142          [-1, 256, 16, 16]             512\n",
      "            ReLU-143          [-1, 256, 16, 16]               0\n",
      "          Conv2d-144            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-145            [-1, 256, 8, 8]             512\n",
      "            ReLU-146            [-1, 256, 8, 8]               0\n",
      "          Conv2d-147           [-1, 1024, 8, 8]         262,144\n",
      "     BatchNorm2d-148           [-1, 1024, 8, 8]           2,048\n",
      "          Conv2d-149           [-1, 1024, 8, 8]         524,288\n",
      "     BatchNorm2d-150           [-1, 1024, 8, 8]           2,048\n",
      "            ReLU-151           [-1, 1024, 8, 8]               0\n",
      "      Bottleneck-152           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-153            [-1, 256, 8, 8]         262,144\n",
      "     BatchNorm2d-154            [-1, 256, 8, 8]             512\n",
      "            ReLU-155            [-1, 256, 8, 8]               0\n",
      "          Conv2d-156            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-157            [-1, 256, 8, 8]             512\n",
      "            ReLU-158            [-1, 256, 8, 8]               0\n",
      "          Conv2d-159           [-1, 1024, 8, 8]         262,144\n",
      "     BatchNorm2d-160           [-1, 1024, 8, 8]           2,048\n",
      "            ReLU-161           [-1, 1024, 8, 8]               0\n",
      "      Bottleneck-162           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-163            [-1, 256, 8, 8]         262,144\n",
      "     BatchNorm2d-164            [-1, 256, 8, 8]             512\n",
      "            ReLU-165            [-1, 256, 8, 8]               0\n",
      "          Conv2d-166            [-1, 256, 8, 8]         589,824\n",
      "     BatchNorm2d-167            [-1, 256, 8, 8]             512\n",
      "            ReLU-168            [-1, 256, 8, 8]               0\n",
      "          Conv2d-169           [-1, 1024, 8, 8]         262,144\n",
      "     BatchNorm2d-170           [-1, 1024, 8, 8]           2,048\n",
      "            ReLU-171           [-1, 1024, 8, 8]               0\n",
      "      Bottleneck-172           [-1, 1024, 8, 8]               0\n",
      "AdaptiveAvgPool2d-173           [-1, 1024, 1, 1]               0\n",
      "          Linear-174                    [-1, 2]           2,050\n",
      "================================================================\n",
      "Total params: 5,890,850\n",
      "Trainable params: 5,890,850\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 187.13\n",
      "Params size (MB): 22.47\n",
      "Estimated Total Size (MB): 209.79\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "class Config:\n",
    "  def __init__(self, **kwargs):\n",
    "    for key, value in kwargs.items():\n",
    "      setattr(self, key, value)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "lr = 0.0008\n",
    "epochs = 30\n",
    "optimizer = 'Adam'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "# 파라미터 클래스\n",
    "config1 = Config(\n",
    "    trainloader = m_train_loader,\n",
    "    testloader = m_valid_loader,\n",
    "    model = resnet50,\n",
    "    device = device,\n",
    "    optimizer = torch.optim.Adam(resnet50.parameters(), lr=lr),\n",
    "    criterion= nn.CrossEntropyLoss().to(device),\n",
    "    globaliter = 0\n",
    ")\n",
    "config2 = Config(\n",
    "    trainloader = g_train_loader,\n",
    "    testloader = g_valid_loader,\n",
    "    model = resnet50_gender,\n",
    "    device = device,\n",
    "    optimizer = torch.optim.Adam(resnet50_gender.parameters(), lr=lr),\n",
    "    criterion= nn.CrossEntropyLoss().to(device),\n",
    "    globaliter = 0\n",
    ")\n",
    "config3 = Config(\n",
    "    trainloader = a_train_loader,\n",
    "    testloader = a_valid_loader,\n",
    "    model = resnet50,\n",
    "    device = device,\n",
    "    optimizer = torch.optim.Adam(resnet50.parameters(), lr=lr),\n",
    "    criterion= nn.CrossEntropyLoss().to(device),\n",
    "    globaliter = 0\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "class train_test():\n",
    "      def __init__(self, config,mode=None):\n",
    "        # 파라미터 인자\n",
    "        self.trainloader = config.trainloader\n",
    "        self.testloader = config.testloader\n",
    "        self.model = config.model\n",
    "        self.device = config.device\n",
    "        self.optimizer = config.optimizer\n",
    "        self.criterion = config.criterion\n",
    "        self.globaliter = config.globaliter\n",
    "        self.min_loss = float(\"inf\")\n",
    "        self.mode = mode\n",
    "      \n",
    "      def train(self, epochs, log_interval, lr_sche):\n",
    "          self.model.train()\n",
    "          for epoch in range(1, epochs + 1 ):  # epochs 루프\n",
    "              running_loss = 0.0\n",
    "              lr_sche.step()\n",
    "              for i, data in enumerate(self.trainloader, 0): # batch 루프\n",
    "                  self.globaliter += 1\n",
    "                  inputs, labels = data # input data, label 분리\n",
    "                  inputs = inputs.to(self.device)\n",
    "                  labels = labels.to(self.device)\n",
    "\n",
    "                  self.optimizer.zero_grad() \n",
    "\n",
    "                  # forward + backward + optimize\n",
    "                  outputs = self.model(inputs)\n",
    "                  loss = self.criterion(outputs, labels)\n",
    "                  loss.backward()\n",
    "                  self.optimizer.step()\n",
    "                  running_loss += loss.item()\n",
    "\n",
    "                  # 30 iteration마다 acc & loss 출력\n",
    "                  if i % log_interval == log_interval -1 : # i는 1에포크의 iteration\n",
    "                    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tlearningLoss: {:.6f}\\twhole_loss: {:.6f} '.format(\n",
    "                        epoch, i*len(inputs), len(self.trainloader.dataset),\n",
    "                        100. * i*len(inputs) / len(self.trainloader.dataset), \n",
    "                        running_loss / log_interval,\n",
    "                        loss.item()))\n",
    "                    running_loss = 0.0\n",
    "\n",
    "              with torch.no_grad():\n",
    "                  self.model.eval()\n",
    "                  correct = 0\n",
    "                  total = 0\n",
    "                  test_loss = 0\n",
    "                  labels_list = []\n",
    "                  predict_list = []\n",
    "                  acc = []\n",
    "                  for k, data in enumerate(self.testloader, 0):\n",
    "                    images, labels = data\n",
    "                   \n",
    "                    images = images.to(self.device)\n",
    "                    labels = labels.to(self.device)\n",
    "                    outputs = self.model(images)\n",
    "\n",
    "                    labels_list.append(labels.detach().cpu().numpy())\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    predict_list.append(predicted.detach().cpu().numpy())\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "                    test_loss += self.criterion(outputs, labels).item()\n",
    "                    acc.append(100 * correct/total)\n",
    "                  \n",
    "                  labels_list = np.concatenate(labels_list)\n",
    "                  predict_list = np.concatenate(predict_list)\n",
    "                  score = f1_score(labels_list, predict_list, average='macro')\n",
    "\n",
    "                  if test_loss < self.min_loss:\n",
    "                    self.min_loss = test_loss\n",
    "                    print(self.mode, \"Minimum ::\",self.min_loss)\n",
    "                    torch.save({\n",
    "                        'model': self.model.state_dict(),\n",
    "                        'optimizer': self.optimizer.state_dict()\n",
    "                    }, PATH + f'{self.mode}_all.tar')\n",
    "                    \n",
    "\n",
    "                  print('\\nTest set : Average loss:{:.4f}, Accuracy: {}/{}({:.0f}%), F1-score: {}\\n'.format(\n",
    "                      test_loss, correct, total, 100 * correct/total, score\n",
    "                  ))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "mask_model = train_test(config1, mode=\"mask\")\n",
    "gender_model= train_test(config2, mode=\"gender\")\n",
    "age_model = train_test(config3, mode=\"ageCate\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "m_lr_sche = optim.lr_scheduler.StepLR(config1.optimizer, step_size=10000, gamma=0.5) # 20 step마다 lr조정\n",
    "g_lr_sche = optim.lr_scheduler.StepLR(config2.optimizer, step_size=10000, gamma=0.5) # 20 step마다 lr조정\n",
    "a_lr_sche = optim.lr_scheduler.StepLR(config3.optimizer, step_size=10000, gamma=0.5) # 20 step마다 lr조정\n",
    "log_interval = 175\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "mask_model.train(epochs, log_interval,m_lr_sche)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train Epoch: 1 [2784/15120 (18%)]\tlearningLoss: 0.014515\twhole_loss: 0.001749 \n",
      "Train Epoch: 1 [5584/15120 (37%)]\tlearningLoss: 0.017357\twhole_loss: 0.002388 \n",
      "Train Epoch: 1 [8384/15120 (55%)]\tlearningLoss: 0.007657\twhole_loss: 0.006423 \n",
      "Train Epoch: 1 [11184/15120 (74%)]\tlearningLoss: 0.008475\twhole_loss: 0.002604 \n",
      "Train Epoch: 1 [13984/15120 (92%)]\tlearningLoss: 0.016981\twhole_loss: 0.001159 \n",
      "mask Minimum :: 8.761815305224104\n",
      "\n",
      "Test set : Average loss:8.7618, Accuracy: 3744/3780(99%), F1-score: 0.9858647811639601\n",
      "\n",
      "Train Epoch: 2 [2784/15120 (18%)]\tlearningLoss: 0.004515\twhole_loss: 0.000001 \n",
      "Train Epoch: 2 [5584/15120 (37%)]\tlearningLoss: 0.008301\twhole_loss: 0.000450 \n",
      "Train Epoch: 2 [8384/15120 (55%)]\tlearningLoss: 0.021842\twhole_loss: 0.104162 \n",
      "Train Epoch: 2 [11184/15120 (74%)]\tlearningLoss: 0.016143\twhole_loss: 0.000009 \n",
      "Train Epoch: 2 [13984/15120 (92%)]\tlearningLoss: 0.010561\twhole_loss: 0.000009 \n",
      "mask Minimum :: 4.572922144206842\n",
      "\n",
      "Test set : Average loss:4.5729, Accuracy: 3765/3780(100%), F1-score: 0.994405657648859\n",
      "\n",
      "Train Epoch: 3 [2784/15120 (18%)]\tlearningLoss: 0.001174\twhole_loss: 0.000001 \n",
      "Train Epoch: 3 [5584/15120 (37%)]\tlearningLoss: 0.005972\twhole_loss: 0.000002 \n",
      "Train Epoch: 3 [8384/15120 (55%)]\tlearningLoss: 0.021076\twhole_loss: 0.001787 \n",
      "Train Epoch: 3 [11184/15120 (74%)]\tlearningLoss: 0.014409\twhole_loss: 0.018410 \n",
      "Train Epoch: 3 [13984/15120 (92%)]\tlearningLoss: 0.000588\twhole_loss: 0.000000 \n",
      "mask Minimum :: 4.5640899456200525\n",
      "\n",
      "Test set : Average loss:4.5641, Accuracy: 3766/3780(100%), F1-score: 0.9942892227797899\n",
      "\n",
      "Train Epoch: 4 [2784/15120 (18%)]\tlearningLoss: 0.027558\twhole_loss: 0.006049 \n",
      "Train Epoch: 4 [5584/15120 (37%)]\tlearningLoss: 0.005484\twhole_loss: 0.000069 \n",
      "Train Epoch: 4 [8384/15120 (55%)]\tlearningLoss: 0.008174\twhole_loss: 0.028199 \n",
      "Train Epoch: 4 [11184/15120 (74%)]\tlearningLoss: 0.001526\twhole_loss: 0.000142 \n",
      "Train Epoch: 4 [13984/15120 (92%)]\tlearningLoss: 0.015585\twhole_loss: 0.015625 \n",
      "\n",
      "Test set : Average loss:7.6104, Accuracy: 3760/3780(99%), F1-score: 0.9920306380587829\n",
      "\n",
      "Train Epoch: 5 [2784/15120 (18%)]\tlearningLoss: 0.001846\twhole_loss: 0.000024 \n",
      "Train Epoch: 5 [5584/15120 (37%)]\tlearningLoss: 0.000382\twhole_loss: 0.000000 \n",
      "Train Epoch: 5 [8384/15120 (55%)]\tlearningLoss: 0.001378\twhole_loss: 0.000254 \n",
      "Train Epoch: 5 [11184/15120 (74%)]\tlearningLoss: 0.008122\twhole_loss: 0.000000 \n",
      "Train Epoch: 5 [13984/15120 (92%)]\tlearningLoss: 0.000413\twhole_loss: 0.000003 \n",
      "\n",
      "Test set : Average loss:5.9060, Accuracy: 3768/3780(100%), F1-score: 0.9955382635181885\n",
      "\n",
      "Train Epoch: 6 [2784/15120 (18%)]\tlearningLoss: 0.000059\twhole_loss: 0.000000 \n",
      "Train Epoch: 6 [5584/15120 (37%)]\tlearningLoss: 0.000041\twhole_loss: 0.000001 \n",
      "Train Epoch: 6 [8384/15120 (55%)]\tlearningLoss: 0.000129\twhole_loss: 0.000000 \n",
      "Train Epoch: 6 [11184/15120 (74%)]\tlearningLoss: 0.000038\twhole_loss: 0.000000 \n",
      "Train Epoch: 6 [13984/15120 (92%)]\tlearningLoss: 0.007578\twhole_loss: 0.000092 \n",
      "\n",
      "Test set : Average loss:7.4090, Accuracy: 3745/3780(99%), F1-score: 0.9862722001265252\n",
      "\n",
      "Train Epoch: 7 [2784/15120 (18%)]\tlearningLoss: 0.009936\twhole_loss: 0.000289 \n",
      "Train Epoch: 7 [5584/15120 (37%)]\tlearningLoss: 0.007357\twhole_loss: 0.000084 \n",
      "Train Epoch: 7 [8384/15120 (55%)]\tlearningLoss: 0.000309\twhole_loss: 0.000011 \n",
      "Train Epoch: 7 [11184/15120 (74%)]\tlearningLoss: 0.043625\twhole_loss: 0.000179 \n",
      "Train Epoch: 7 [13984/15120 (92%)]\tlearningLoss: 0.008132\twhole_loss: 0.000053 \n",
      "\n",
      "Test set : Average loss:10.9862, Accuracy: 3740/3780(99%), F1-score: 0.9845891916920756\n",
      "\n",
      "Train Epoch: 8 [2784/15120 (18%)]\tlearningLoss: 0.006012\twhole_loss: 0.000015 \n",
      "Train Epoch: 8 [5584/15120 (37%)]\tlearningLoss: 0.001720\twhole_loss: 0.000008 \n",
      "Train Epoch: 8 [8384/15120 (55%)]\tlearningLoss: 0.000169\twhole_loss: 0.000029 \n",
      "Train Epoch: 8 [11184/15120 (74%)]\tlearningLoss: 0.000571\twhole_loss: 0.000000 \n",
      "Train Epoch: 8 [13984/15120 (92%)]\tlearningLoss: 0.000185\twhole_loss: 0.001355 \n",
      "\n",
      "Test set : Average loss:6.0769, Accuracy: 3762/3780(100%), F1-score: 0.993075649410129\n",
      "\n",
      "Train Epoch: 9 [2784/15120 (18%)]\tlearningLoss: 0.013309\twhole_loss: 0.000128 \n",
      "Train Epoch: 9 [5584/15120 (37%)]\tlearningLoss: 0.002163\twhole_loss: 0.017094 \n",
      "Train Epoch: 9 [8384/15120 (55%)]\tlearningLoss: 0.009133\twhole_loss: 0.000009 \n",
      "Train Epoch: 9 [11184/15120 (74%)]\tlearningLoss: 0.004667\twhole_loss: 0.000047 \n",
      "Train Epoch: 9 [13984/15120 (92%)]\tlearningLoss: 0.007212\twhole_loss: 0.000401 \n",
      "\n",
      "Test set : Average loss:6.8113, Accuracy: 3761/3780(99%), F1-score: 0.9921810191147701\n",
      "\n",
      "Train Epoch: 10 [2784/15120 (18%)]\tlearningLoss: 0.008916\twhole_loss: 0.000000 \n",
      "Train Epoch: 10 [5584/15120 (37%)]\tlearningLoss: 0.004546\twhole_loss: 0.000489 \n",
      "Train Epoch: 10 [8384/15120 (55%)]\tlearningLoss: 0.000467\twhole_loss: 0.000563 \n",
      "Train Epoch: 10 [11184/15120 (74%)]\tlearningLoss: 0.008357\twhole_loss: 0.039446 \n",
      "Train Epoch: 10 [13984/15120 (92%)]\tlearningLoss: 0.006424\twhole_loss: 0.000000 \n",
      "\n",
      "Test set : Average loss:7.3549, Accuracy: 3760/3780(99%), F1-score: 0.9917929907413979\n",
      "\n",
      "Train Epoch: 11 [2784/15120 (18%)]\tlearningLoss: 0.017416\twhole_loss: 0.104069 \n",
      "Train Epoch: 11 [5584/15120 (37%)]\tlearningLoss: 0.009794\twhole_loss: 0.000225 \n",
      "Train Epoch: 11 [8384/15120 (55%)]\tlearningLoss: 0.005564\twhole_loss: 0.000032 \n",
      "Train Epoch: 11 [11184/15120 (74%)]\tlearningLoss: 0.003721\twhole_loss: 0.000031 \n",
      "Train Epoch: 11 [13984/15120 (92%)]\tlearningLoss: 0.002662\twhole_loss: 0.000038 \n",
      "\n",
      "Test set : Average loss:6.9275, Accuracy: 3759/3780(99%), F1-score: 0.9916622781208758\n",
      "\n",
      "Train Epoch: 12 [2784/15120 (18%)]\tlearningLoss: 0.000312\twhole_loss: 0.000000 \n",
      "Train Epoch: 12 [5584/15120 (37%)]\tlearningLoss: 0.000893\twhole_loss: 0.000031 \n",
      "Train Epoch: 12 [8384/15120 (55%)]\tlearningLoss: 0.000435\twhole_loss: 0.000000 \n",
      "Train Epoch: 12 [11184/15120 (74%)]\tlearningLoss: 0.000021\twhole_loss: 0.000001 \n",
      "Train Epoch: 12 [13984/15120 (92%)]\tlearningLoss: 0.000142\twhole_loss: 0.000000 \n",
      "\n",
      "Test set : Average loss:6.6075, Accuracy: 3764/3780(100%), F1-score: 0.9937897966852977\n",
      "\n",
      "Train Epoch: 13 [2784/15120 (18%)]\tlearningLoss: 0.000008\twhole_loss: 0.000131 \n",
      "Train Epoch: 13 [5584/15120 (37%)]\tlearningLoss: 0.000009\twhole_loss: 0.000000 \n",
      "Train Epoch: 13 [8384/15120 (55%)]\tlearningLoss: 0.000037\twhole_loss: 0.000000 \n",
      "Train Epoch: 13 [11184/15120 (74%)]\tlearningLoss: 0.000010\twhole_loss: 0.000000 \n",
      "Train Epoch: 13 [13984/15120 (92%)]\tlearningLoss: 0.000018\twhole_loss: 0.000000 \n",
      "\n",
      "Test set : Average loss:7.1443, Accuracy: 3763/3780(100%), F1-score: 0.9934124636611091\n",
      "\n",
      "Train Epoch: 14 [2784/15120 (18%)]\tlearningLoss: 0.000019\twhole_loss: 0.000001 \n",
      "Train Epoch: 14 [5584/15120 (37%)]\tlearningLoss: 0.000002\twhole_loss: 0.000000 \n",
      "Train Epoch: 14 [8384/15120 (55%)]\tlearningLoss: 0.000005\twhole_loss: 0.000003 \n",
      "Train Epoch: 14 [11184/15120 (74%)]\tlearningLoss: 0.000002\twhole_loss: 0.000001 \n",
      "Train Epoch: 14 [13984/15120 (92%)]\tlearningLoss: 0.000006\twhole_loss: 0.000000 \n",
      "\n",
      "Test set : Average loss:7.3231, Accuracy: 3764/3780(100%), F1-score: 0.9937897966852977\n",
      "\n",
      "Train Epoch: 15 [2784/15120 (18%)]\tlearningLoss: 0.000002\twhole_loss: 0.000000 \n",
      "Train Epoch: 15 [5584/15120 (37%)]\tlearningLoss: 0.000003\twhole_loss: 0.000001 \n",
      "Train Epoch: 15 [8384/15120 (55%)]\tlearningLoss: 0.000003\twhole_loss: 0.000000 \n",
      "Train Epoch: 15 [11184/15120 (74%)]\tlearningLoss: 0.000003\twhole_loss: 0.000000 \n",
      "Train Epoch: 15 [13984/15120 (92%)]\tlearningLoss: 0.000001\twhole_loss: 0.000000 \n",
      "\n",
      "Test set : Average loss:7.8054, Accuracy: 3765/3780(100%), F1-score: 0.9941587452096431\n",
      "\n",
      "Train Epoch: 16 [2784/15120 (18%)]\tlearningLoss: 0.000001\twhole_loss: 0.000000 \n",
      "Train Epoch: 16 [5584/15120 (37%)]\tlearningLoss: 0.000002\twhole_loss: 0.000000 \n",
      "Train Epoch: 16 [8384/15120 (55%)]\tlearningLoss: 0.000001\twhole_loss: 0.000000 \n",
      "Train Epoch: 16 [11184/15120 (74%)]\tlearningLoss: 0.000001\twhole_loss: 0.000000 \n",
      "Train Epoch: 16 [13984/15120 (92%)]\tlearningLoss: 0.000004\twhole_loss: 0.000000 \n",
      "\n",
      "Test set : Average loss:7.9094, Accuracy: 3765/3780(100%), F1-score: 0.9941587452096431\n",
      "\n",
      "Train Epoch: 17 [2784/15120 (18%)]\tlearningLoss: 0.000001\twhole_loss: 0.000000 \n",
      "Train Epoch: 17 [5584/15120 (37%)]\tlearningLoss: 0.000001\twhole_loss: 0.000000 \n",
      "Train Epoch: 17 [8384/15120 (55%)]\tlearningLoss: 0.000001\twhole_loss: 0.000002 \n",
      "Train Epoch: 17 [11184/15120 (74%)]\tlearningLoss: 0.000002\twhole_loss: 0.000000 \n",
      "Train Epoch: 17 [13984/15120 (92%)]\tlearningLoss: 0.000001\twhole_loss: 0.000001 \n",
      "\n",
      "Test set : Average loss:8.2662, Accuracy: 3765/3780(100%), F1-score: 0.9941587452096431\n",
      "\n",
      "Train Epoch: 18 [2784/15120 (18%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "Train Epoch: 18 [5584/15120 (37%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "Train Epoch: 18 [8384/15120 (55%)]\tlearningLoss: 0.000001\twhole_loss: 0.000001 \n",
      "Train Epoch: 18 [11184/15120 (74%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "Train Epoch: 18 [13984/15120 (92%)]\tlearningLoss: 0.000001\twhole_loss: 0.000000 \n",
      "\n",
      "Test set : Average loss:8.5614, Accuracy: 3765/3780(100%), F1-score: 0.9941587452096431\n",
      "\n",
      "Train Epoch: 19 [2784/15120 (18%)]\tlearningLoss: 0.000000\twhole_loss: 0.000001 \n",
      "Train Epoch: 19 [5584/15120 (37%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "Train Epoch: 19 [8384/15120 (55%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "Train Epoch: 19 [11184/15120 (74%)]\tlearningLoss: 0.000001\twhole_loss: 0.000000 \n",
      "Train Epoch: 19 [13984/15120 (92%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "\n",
      "Test set : Average loss:8.8017, Accuracy: 3765/3780(100%), F1-score: 0.9941587452096431\n",
      "\n",
      "Train Epoch: 20 [2784/15120 (18%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "Train Epoch: 20 [5584/15120 (37%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "Train Epoch: 20 [8384/15120 (55%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "Train Epoch: 20 [11184/15120 (74%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "Train Epoch: 20 [13984/15120 (92%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "\n",
      "Test set : Average loss:9.0791, Accuracy: 3765/3780(100%), F1-score: 0.9941587452096431\n",
      "\n",
      "Train Epoch: 21 [2784/15120 (18%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "Train Epoch: 21 [5584/15120 (37%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "Train Epoch: 21 [8384/15120 (55%)]\tlearningLoss: 0.000000\twhole_loss: 0.000001 \n",
      "Train Epoch: 21 [11184/15120 (74%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "Train Epoch: 21 [13984/15120 (92%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "\n",
      "Test set : Average loss:9.3473, Accuracy: 3765/3780(100%), F1-score: 0.9941587452096431\n",
      "\n",
      "Train Epoch: 22 [2784/15120 (18%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "Train Epoch: 22 [5584/15120 (37%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "Train Epoch: 22 [8384/15120 (55%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "Train Epoch: 22 [11184/15120 (74%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "Train Epoch: 22 [13984/15120 (92%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "\n",
      "Test set : Average loss:9.5924, Accuracy: 3765/3780(100%), F1-score: 0.9941587452096431\n",
      "\n",
      "Train Epoch: 23 [2784/15120 (18%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "Train Epoch: 23 [5584/15120 (37%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "Train Epoch: 23 [8384/15120 (55%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "Train Epoch: 23 [11184/15120 (74%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "Train Epoch: 23 [13984/15120 (92%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "\n",
      "Test set : Average loss:10.2861, Accuracy: 3765/3780(100%), F1-score: 0.9941587452096431\n",
      "\n",
      "Train Epoch: 24 [2784/15120 (18%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "Train Epoch: 24 [5584/15120 (37%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "Train Epoch: 24 [8384/15120 (55%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "Train Epoch: 24 [11184/15120 (74%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "Train Epoch: 24 [13984/15120 (92%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "\n",
      "Test set : Average loss:10.3835, Accuracy: 3765/3780(100%), F1-score: 0.9941587452096431\n",
      "\n",
      "Train Epoch: 25 [2784/15120 (18%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "Train Epoch: 25 [5584/15120 (37%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "Train Epoch: 25 [8384/15120 (55%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "Train Epoch: 25 [11184/15120 (74%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "Train Epoch: 25 [13984/15120 (92%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "\n",
      "Test set : Average loss:10.5983, Accuracy: 3765/3780(100%), F1-score: 0.9941587452096431\n",
      "\n",
      "Train Epoch: 26 [2784/15120 (18%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "Train Epoch: 26 [5584/15120 (37%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "Train Epoch: 26 [8384/15120 (55%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "Train Epoch: 26 [11184/15120 (74%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "Train Epoch: 26 [13984/15120 (92%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "\n",
      "Test set : Average loss:10.8614, Accuracy: 3765/3780(100%), F1-score: 0.9941587452096431\n",
      "\n",
      "Train Epoch: 27 [2784/15120 (18%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "Train Epoch: 27 [5584/15120 (37%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "Train Epoch: 27 [8384/15120 (55%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "Train Epoch: 27 [11184/15120 (74%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "Train Epoch: 27 [13984/15120 (92%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "\n",
      "Test set : Average loss:11.1386, Accuracy: 3764/3780(100%), F1-score: 0.9937897966852977\n",
      "\n",
      "Train Epoch: 28 [2784/15120 (18%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "Train Epoch: 28 [5584/15120 (37%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "Train Epoch: 28 [8384/15120 (55%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "Train Epoch: 28 [11184/15120 (74%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "Train Epoch: 28 [13984/15120 (92%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "\n",
      "Test set : Average loss:11.2891, Accuracy: 3765/3780(100%), F1-score: 0.9944134528655967\n",
      "\n",
      "Train Epoch: 29 [2784/15120 (18%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "Train Epoch: 29 [5584/15120 (37%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "Train Epoch: 29 [8384/15120 (55%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "Train Epoch: 29 [11184/15120 (74%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "Train Epoch: 29 [13984/15120 (92%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "\n",
      "Test set : Average loss:11.6951, Accuracy: 3764/3780(100%), F1-score: 0.9937897966852977\n",
      "\n",
      "Train Epoch: 30 [2784/15120 (18%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "Train Epoch: 30 [5584/15120 (37%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "Train Epoch: 30 [8384/15120 (55%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "Train Epoch: 30 [11184/15120 (74%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "Train Epoch: 30 [13984/15120 (92%)]\tlearningLoss: 0.000000\twhole_loss: 0.000000 \n",
      "\n",
      "Test set : Average loss:11.7546, Accuracy: 3765/3780(100%), F1-score: 0.9944134528655967\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "gender_model.train(epochs, log_interval,g_lr_sche)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train Epoch: 1 [2784/15120 (18%)]\tlearningLoss: 0.034413\twhole_loss: 0.005132 \n",
      "Train Epoch: 1 [5584/15120 (37%)]\tlearningLoss: 0.037141\twhole_loss: 0.008287 \n",
      "Train Epoch: 1 [8384/15120 (55%)]\tlearningLoss: 0.029943\twhole_loss: 0.256830 \n",
      "Train Epoch: 1 [11184/15120 (74%)]\tlearningLoss: 0.028811\twhole_loss: 0.010416 \n",
      "Train Epoch: 1 [13984/15120 (92%)]\tlearningLoss: 0.059230\twhole_loss: 0.021733 \n",
      "gender Minimum :: 37.93166743044276\n",
      "\n",
      "Test set : Average loss:37.9317, Accuracy: 3620/3780(96%), F1-score: 0.9558985538400779\n",
      "\n",
      "Train Epoch: 2 [2784/15120 (18%)]\tlearningLoss: 0.065627\twhole_loss: 0.011559 \n",
      "Train Epoch: 2 [5584/15120 (37%)]\tlearningLoss: 0.065409\twhole_loss: 0.043910 \n",
      "Train Epoch: 2 [8384/15120 (55%)]\tlearningLoss: 0.041438\twhole_loss: 0.157047 \n",
      "Train Epoch: 2 [11184/15120 (74%)]\tlearningLoss: 0.055116\twhole_loss: 0.001200 \n",
      "Train Epoch: 2 [13984/15120 (92%)]\tlearningLoss: 0.046219\twhole_loss: 0.124955 \n",
      "\n",
      "Test set : Average loss:42.5523, Accuracy: 3595/3780(95%), F1-score: 0.9494263510479477\n",
      "\n",
      "Train Epoch: 3 [2784/15120 (18%)]\tlearningLoss: 0.025808\twhole_loss: 0.006048 \n",
      "Train Epoch: 3 [5584/15120 (37%)]\tlearningLoss: 0.039159\twhole_loss: 0.147334 \n",
      "Train Epoch: 3 [8384/15120 (55%)]\tlearningLoss: 0.048335\twhole_loss: 0.010285 \n",
      "Train Epoch: 3 [11184/15120 (74%)]\tlearningLoss: 0.036501\twhole_loss: 0.002036 \n",
      "Train Epoch: 3 [13984/15120 (92%)]\tlearningLoss: 0.037437\twhole_loss: 0.050909 \n",
      "\n",
      "Test set : Average loss:55.3346, Accuracy: 3601/3780(95%), F1-score: 0.9502039729519745\n",
      "\n",
      "Train Epoch: 4 [2784/15120 (18%)]\tlearningLoss: 0.030894\twhole_loss: 0.113382 \n",
      "Train Epoch: 4 [5584/15120 (37%)]\tlearningLoss: 0.028961\twhole_loss: 0.000028 \n",
      "Train Epoch: 4 [8384/15120 (55%)]\tlearningLoss: 0.035077\twhole_loss: 0.023229 \n",
      "Train Epoch: 4 [11184/15120 (74%)]\tlearningLoss: 0.014088\twhole_loss: 0.006766 \n",
      "Train Epoch: 4 [13984/15120 (92%)]\tlearningLoss: 0.026528\twhole_loss: 0.006258 \n",
      "\n",
      "Test set : Average loss:68.6134, Accuracy: 3551/3780(94%), F1-score: 0.9356647437637826\n",
      "\n",
      "Train Epoch: 5 [2784/15120 (18%)]\tlearningLoss: 0.041783\twhole_loss: 0.008200 \n",
      "Train Epoch: 5 [5584/15120 (37%)]\tlearningLoss: 0.044991\twhole_loss: 0.042542 \n",
      "Train Epoch: 5 [8384/15120 (55%)]\tlearningLoss: 0.029347\twhole_loss: 0.047017 \n",
      "Train Epoch: 5 [11184/15120 (74%)]\tlearningLoss: 0.030365\twhole_loss: 0.004539 \n",
      "Train Epoch: 5 [13984/15120 (92%)]\tlearningLoss: 0.019765\twhole_loss: 0.112458 \n",
      "\n",
      "Test set : Average loss:48.7407, Accuracy: 3624/3780(96%), F1-score: 0.9567025442468455\n",
      "\n",
      "Train Epoch: 6 [2784/15120 (18%)]\tlearningLoss: 0.023372\twhole_loss: 0.000025 \n",
      "Train Epoch: 6 [5584/15120 (37%)]\tlearningLoss: 0.035513\twhole_loss: 0.019638 \n",
      "Train Epoch: 6 [8384/15120 (55%)]\tlearningLoss: 0.025128\twhole_loss: 0.000020 \n",
      "Train Epoch: 6 [11184/15120 (74%)]\tlearningLoss: 0.016031\twhole_loss: 0.068869 \n",
      "Train Epoch: 6 [13984/15120 (92%)]\tlearningLoss: 0.013235\twhole_loss: 0.030689 \n",
      "\n",
      "Test set : Average loss:53.0704, Accuracy: 3579/3780(95%), F1-score: 0.9440144054328825\n",
      "\n",
      "Train Epoch: 7 [2784/15120 (18%)]\tlearningLoss: 0.022513\twhole_loss: 0.001565 \n",
      "Train Epoch: 7 [5584/15120 (37%)]\tlearningLoss: 0.016755\twhole_loss: 0.151304 \n",
      "Train Epoch: 7 [8384/15120 (55%)]\tlearningLoss: 0.034938\twhole_loss: 0.004420 \n",
      "Train Epoch: 7 [11184/15120 (74%)]\tlearningLoss: 0.024434\twhole_loss: 0.000141 \n",
      "Train Epoch: 7 [13984/15120 (92%)]\tlearningLoss: 0.040997\twhole_loss: 0.094335 \n",
      "\n",
      "Test set : Average loss:52.3464, Accuracy: 3601/3780(95%), F1-score: 0.9500159233657275\n",
      "\n",
      "Train Epoch: 8 [2784/15120 (18%)]\tlearningLoss: 0.012066\twhole_loss: 0.002001 \n",
      "Train Epoch: 8 [5584/15120 (37%)]\tlearningLoss: 0.007094\twhole_loss: 0.001525 \n",
      "Train Epoch: 8 [8384/15120 (55%)]\tlearningLoss: 0.018272\twhole_loss: 0.000026 \n",
      "Train Epoch: 8 [11184/15120 (74%)]\tlearningLoss: 0.015913\twhole_loss: 0.006448 \n",
      "Train Epoch: 8 [13984/15120 (92%)]\tlearningLoss: 0.019530\twhole_loss: 0.002628 \n",
      "\n",
      "Test set : Average loss:67.7403, Accuracy: 3578/3780(95%), F1-score: 0.9434985940506142\n",
      "\n",
      "Train Epoch: 9 [2784/15120 (18%)]\tlearningLoss: 0.016331\twhole_loss: 0.000012 \n",
      "Train Epoch: 9 [5584/15120 (37%)]\tlearningLoss: 0.018235\twhole_loss: 0.000134 \n",
      "Train Epoch: 9 [8384/15120 (55%)]\tlearningLoss: 0.020142\twhole_loss: 0.087997 \n",
      "Train Epoch: 9 [11184/15120 (74%)]\tlearningLoss: 0.020529\twhole_loss: 0.001579 \n",
      "Train Epoch: 9 [13984/15120 (92%)]\tlearningLoss: 0.043499\twhole_loss: 0.026507 \n",
      "\n",
      "Test set : Average loss:79.3950, Accuracy: 3555/3780(94%), F1-score: 0.9389846661040773\n",
      "\n",
      "Train Epoch: 10 [2784/15120 (18%)]\tlearningLoss: 0.014944\twhole_loss: 0.001154 \n",
      "Train Epoch: 10 [5584/15120 (37%)]\tlearningLoss: 0.017280\twhole_loss: 0.000392 \n",
      "Train Epoch: 10 [8384/15120 (55%)]\tlearningLoss: 0.026358\twhole_loss: 0.006722 \n",
      "Train Epoch: 10 [11184/15120 (74%)]\tlearningLoss: 0.014413\twhole_loss: 0.000885 \n",
      "Train Epoch: 10 [13984/15120 (92%)]\tlearningLoss: 0.039101\twhole_loss: 0.004655 \n",
      "\n",
      "Test set : Average loss:52.1486, Accuracy: 3590/3780(95%), F1-score: 0.9471373654181507\n",
      "\n",
      "Train Epoch: 11 [2784/15120 (18%)]\tlearningLoss: 0.007366\twhole_loss: 0.000055 \n",
      "Train Epoch: 11 [5584/15120 (37%)]\tlearningLoss: 0.019296\twhole_loss: 0.002387 \n",
      "Train Epoch: 11 [8384/15120 (55%)]\tlearningLoss: 0.007229\twhole_loss: 0.306354 \n",
      "Train Epoch: 11 [11184/15120 (74%)]\tlearningLoss: 0.029379\twhole_loss: 0.000462 \n",
      "Train Epoch: 11 [13984/15120 (92%)]\tlearningLoss: 0.018279\twhole_loss: 0.098015 \n",
      "\n",
      "Test set : Average loss:67.8035, Accuracy: 3602/3780(95%), F1-score: 0.9505004250766065\n",
      "\n",
      "Train Epoch: 12 [2784/15120 (18%)]\tlearningLoss: 0.002009\twhole_loss: 0.000055 \n",
      "Train Epoch: 12 [5584/15120 (37%)]\tlearningLoss: 0.005984\twhole_loss: 0.000051 \n",
      "Train Epoch: 12 [8384/15120 (55%)]\tlearningLoss: 0.035879\twhole_loss: 0.001674 \n",
      "Train Epoch: 12 [11184/15120 (74%)]\tlearningLoss: 0.025504\twhole_loss: 0.013535 \n",
      "Train Epoch: 12 [13984/15120 (92%)]\tlearningLoss: 0.008503\twhole_loss: 0.073958 \n",
      "\n",
      "Test set : Average loss:60.9899, Accuracy: 3605/3780(95%), F1-score: 0.9513406550629147\n",
      "\n",
      "Train Epoch: 13 [2784/15120 (18%)]\tlearningLoss: 0.028269\twhole_loss: 0.117953 \n",
      "Train Epoch: 13 [5584/15120 (37%)]\tlearningLoss: 0.025373\twhole_loss: 0.000008 \n",
      "Train Epoch: 13 [8384/15120 (55%)]\tlearningLoss: 0.011419\twhole_loss: 0.001136 \n",
      "Train Epoch: 13 [11184/15120 (74%)]\tlearningLoss: 0.005516\twhole_loss: 0.000308 \n",
      "Train Epoch: 13 [13984/15120 (92%)]\tlearningLoss: 0.026113\twhole_loss: 0.085059 \n",
      "\n",
      "Test set : Average loss:66.8924, Accuracy: 3582/3780(95%), F1-score: 0.9445301531134974\n",
      "\n",
      "Train Epoch: 14 [2784/15120 (18%)]\tlearningLoss: 0.014118\twhole_loss: 0.001121 \n",
      "Train Epoch: 14 [5584/15120 (37%)]\tlearningLoss: 0.028740\twhole_loss: 0.002301 \n",
      "Train Epoch: 14 [8384/15120 (55%)]\tlearningLoss: 0.015808\twhole_loss: 0.016912 \n",
      "Train Epoch: 14 [11184/15120 (74%)]\tlearningLoss: 0.014870\twhole_loss: 0.000067 \n",
      "Train Epoch: 14 [13984/15120 (92%)]\tlearningLoss: 0.020671\twhole_loss: 0.009865 \n",
      "\n",
      "Test set : Average loss:61.6300, Accuracy: 3624/3780(96%), F1-score: 0.9572120681588361\n",
      "\n",
      "Train Epoch: 15 [2784/15120 (18%)]\tlearningLoss: 0.004409\twhole_loss: 0.000011 \n",
      "Train Epoch: 15 [5584/15120 (37%)]\tlearningLoss: 0.004787\twhole_loss: 0.000004 \n",
      "Train Epoch: 15 [8384/15120 (55%)]\tlearningLoss: 0.022413\twhole_loss: 0.000060 \n",
      "Train Epoch: 15 [11184/15120 (74%)]\tlearningLoss: 0.028059\twhole_loss: 0.002380 \n",
      "Train Epoch: 15 [13984/15120 (92%)]\tlearningLoss: 0.014252\twhole_loss: 0.000098 \n",
      "\n",
      "Test set : Average loss:55.1642, Accuracy: 3626/3780(96%), F1-score: 0.9575429588353706\n",
      "\n",
      "Train Epoch: 16 [2784/15120 (18%)]\tlearningLoss: 0.025836\twhole_loss: 0.002528 \n",
      "Train Epoch: 16 [5584/15120 (37%)]\tlearningLoss: 0.008633\twhole_loss: 0.000038 \n",
      "Train Epoch: 16 [8384/15120 (55%)]\tlearningLoss: 0.007642\twhole_loss: 0.000077 \n",
      "Train Epoch: 16 [11184/15120 (74%)]\tlearningLoss: 0.010282\twhole_loss: 0.001364 \n",
      "Train Epoch: 16 [13984/15120 (92%)]\tlearningLoss: 0.039042\twhole_loss: 0.199883 \n",
      "\n",
      "Test set : Average loss:49.2208, Accuracy: 3639/3780(96%), F1-score: 0.9609811479162089\n",
      "\n",
      "Train Epoch: 17 [2784/15120 (18%)]\tlearningLoss: 0.008209\twhole_loss: 0.000326 \n",
      "Train Epoch: 17 [5584/15120 (37%)]\tlearningLoss: 0.011758\twhole_loss: 0.043372 \n",
      "Train Epoch: 17 [8384/15120 (55%)]\tlearningLoss: 0.011963\twhole_loss: 0.000175 \n",
      "Train Epoch: 17 [11184/15120 (74%)]\tlearningLoss: 0.006796\twhole_loss: 0.000005 \n",
      "Train Epoch: 17 [13984/15120 (92%)]\tlearningLoss: 0.025598\twhole_loss: 0.000015 \n",
      "\n",
      "Test set : Average loss:46.2027, Accuracy: 3647/3780(96%), F1-score: 0.9632625439077053\n",
      "\n",
      "Train Epoch: 18 [2784/15120 (18%)]\tlearningLoss: 0.001712\twhole_loss: 0.000119 \n",
      "Train Epoch: 18 [5584/15120 (37%)]\tlearningLoss: 0.025282\twhole_loss: 0.011125 \n",
      "Train Epoch: 18 [8384/15120 (55%)]\tlearningLoss: 0.009175\twhole_loss: 0.000205 \n",
      "Train Epoch: 18 [11184/15120 (74%)]\tlearningLoss: 0.009414\twhole_loss: 0.000849 \n",
      "Train Epoch: 18 [13984/15120 (92%)]\tlearningLoss: 0.014876\twhole_loss: 0.040527 \n",
      "\n",
      "Test set : Average loss:55.6055, Accuracy: 3651/3780(97%), F1-score: 0.9642517159535571\n",
      "\n",
      "Train Epoch: 19 [2784/15120 (18%)]\tlearningLoss: 0.012073\twhole_loss: 0.003873 \n",
      "Train Epoch: 19 [5584/15120 (37%)]\tlearningLoss: 0.018129\twhole_loss: 0.000740 \n",
      "Train Epoch: 19 [8384/15120 (55%)]\tlearningLoss: 0.021212\twhole_loss: 0.381286 \n",
      "Train Epoch: 19 [11184/15120 (74%)]\tlearningLoss: 0.013901\twhole_loss: 0.000026 \n",
      "Train Epoch: 19 [13984/15120 (92%)]\tlearningLoss: 0.003140\twhole_loss: 0.054122 \n",
      "\n",
      "Test set : Average loss:50.9960, Accuracy: 3595/3780(95%), F1-score: 0.9490133354953876\n",
      "\n",
      "Train Epoch: 20 [2784/15120 (18%)]\tlearningLoss: 0.014825\twhole_loss: 0.000493 \n",
      "Train Epoch: 20 [5584/15120 (37%)]\tlearningLoss: 0.019312\twhole_loss: 0.000419 \n",
      "Train Epoch: 20 [8384/15120 (55%)]\tlearningLoss: 0.003962\twhole_loss: 0.000065 \n",
      "Train Epoch: 20 [11184/15120 (74%)]\tlearningLoss: 0.000732\twhole_loss: 0.000002 \n",
      "Train Epoch: 20 [13984/15120 (92%)]\tlearningLoss: 0.008671\twhole_loss: 0.000134 \n",
      "\n",
      "Test set : Average loss:50.1671, Accuracy: 3596/3780(95%), F1-score: 0.9495321829565759\n",
      "\n",
      "Train Epoch: 21 [2784/15120 (18%)]\tlearningLoss: 0.004843\twhole_loss: 0.000077 \n",
      "Train Epoch: 21 [5584/15120 (37%)]\tlearningLoss: 0.006411\twhole_loss: 0.000118 \n",
      "Train Epoch: 21 [8384/15120 (55%)]\tlearningLoss: 0.042914\twhole_loss: 0.011837 \n",
      "Train Epoch: 21 [11184/15120 (74%)]\tlearningLoss: 0.014017\twhole_loss: 0.000323 \n",
      "Train Epoch: 21 [13984/15120 (92%)]\tlearningLoss: 0.004294\twhole_loss: 0.000745 \n",
      "\n",
      "Test set : Average loss:77.5140, Accuracy: 3558/3780(94%), F1-score: 0.9376397821271496\n",
      "\n",
      "Train Epoch: 22 [2784/15120 (18%)]\tlearningLoss: 0.009795\twhole_loss: 0.000724 \n",
      "Train Epoch: 22 [5584/15120 (37%)]\tlearningLoss: 0.012183\twhole_loss: 0.000067 \n",
      "Train Epoch: 22 [8384/15120 (55%)]\tlearningLoss: 0.009765\twhole_loss: 0.000475 \n",
      "Train Epoch: 22 [11184/15120 (74%)]\tlearningLoss: 0.030833\twhole_loss: 0.230840 \n",
      "Train Epoch: 22 [13984/15120 (92%)]\tlearningLoss: 0.005742\twhole_loss: 0.000293 \n",
      "\n",
      "Test set : Average loss:51.5076, Accuracy: 3642/3780(96%), F1-score: 0.9620453098847215\n",
      "\n",
      "Train Epoch: 23 [2784/15120 (18%)]\tlearningLoss: 0.001260\twhole_loss: 0.000051 \n",
      "Train Epoch: 23 [5584/15120 (37%)]\tlearningLoss: 0.000364\twhole_loss: 0.000078 \n",
      "Train Epoch: 23 [8384/15120 (55%)]\tlearningLoss: 0.001222\twhole_loss: 0.000759 \n",
      "Train Epoch: 23 [11184/15120 (74%)]\tlearningLoss: 0.000370\twhole_loss: 0.000007 \n",
      "Train Epoch: 23 [13984/15120 (92%)]\tlearningLoss: 0.000282\twhole_loss: 0.000001 \n",
      "\n",
      "Test set : Average loss:61.1661, Accuracy: 3586/3780(95%), F1-score: 0.9465388007054674\n",
      "\n",
      "Train Epoch: 24 [2784/15120 (18%)]\tlearningLoss: 0.035421\twhole_loss: 0.011379 \n",
      "Train Epoch: 24 [5584/15120 (37%)]\tlearningLoss: 0.031053\twhole_loss: 0.001624 \n",
      "Train Epoch: 24 [8384/15120 (55%)]\tlearningLoss: 0.010872\twhole_loss: 0.083316 \n",
      "Train Epoch: 24 [11184/15120 (74%)]\tlearningLoss: 0.004206\twhole_loss: 0.000001 \n",
      "Train Epoch: 24 [13984/15120 (92%)]\tlearningLoss: 0.001059\twhole_loss: 0.000014 \n",
      "\n",
      "Test set : Average loss:66.5197, Accuracy: 3635/3780(96%), F1-score: 0.9599478862151674\n",
      "\n",
      "Train Epoch: 25 [2784/15120 (18%)]\tlearningLoss: 0.000097\twhole_loss: 0.000169 \n",
      "Train Epoch: 25 [5584/15120 (37%)]\tlearningLoss: 0.000064\twhole_loss: 0.000000 \n",
      "Train Epoch: 25 [8384/15120 (55%)]\tlearningLoss: 0.000074\twhole_loss: 0.000014 \n",
      "Train Epoch: 25 [11184/15120 (74%)]\tlearningLoss: 0.000315\twhole_loss: 0.000004 \n",
      "Train Epoch: 25 [13984/15120 (92%)]\tlearningLoss: 0.036399\twhole_loss: 0.000141 \n",
      "\n",
      "Test set : Average loss:100.5074, Accuracy: 3481/3780(92%), F1-score: 0.915111882916858\n",
      "\n",
      "Train Epoch: 26 [2784/15120 (18%)]\tlearningLoss: 0.014365\twhole_loss: 0.001743 \n",
      "Train Epoch: 26 [5584/15120 (37%)]\tlearningLoss: 0.025930\twhole_loss: 0.371104 \n",
      "Train Epoch: 26 [8384/15120 (55%)]\tlearningLoss: 0.023415\twhole_loss: 0.000236 \n",
      "Train Epoch: 26 [11184/15120 (74%)]\tlearningLoss: 0.006885\twhole_loss: 0.000064 \n",
      "Train Epoch: 26 [13984/15120 (92%)]\tlearningLoss: 0.004002\twhole_loss: 0.000010 \n",
      "\n",
      "Test set : Average loss:54.3401, Accuracy: 3633/3780(96%), F1-score: 0.9592635832959138\n",
      "\n",
      "Train Epoch: 27 [2784/15120 (18%)]\tlearningLoss: 0.014774\twhole_loss: 0.000010 \n",
      "Train Epoch: 27 [5584/15120 (37%)]\tlearningLoss: 0.016367\twhole_loss: 0.000001 \n",
      "Train Epoch: 27 [8384/15120 (55%)]\tlearningLoss: 0.001490\twhole_loss: 0.000110 \n",
      "Train Epoch: 27 [11184/15120 (74%)]\tlearningLoss: 0.000928\twhole_loss: 0.001505 \n",
      "Train Epoch: 27 [13984/15120 (92%)]\tlearningLoss: 0.024968\twhole_loss: 0.000777 \n",
      "\n",
      "Test set : Average loss:72.6482, Accuracy: 3582/3780(95%), F1-score: 0.9444115004705904\n",
      "\n",
      "Train Epoch: 28 [2784/15120 (18%)]\tlearningLoss: 0.001927\twhole_loss: 0.000108 \n",
      "Train Epoch: 28 [5584/15120 (37%)]\tlearningLoss: 0.001367\twhole_loss: 0.172348 \n",
      "Train Epoch: 28 [8384/15120 (55%)]\tlearningLoss: 0.037197\twhole_loss: 0.003859 \n",
      "Train Epoch: 28 [11184/15120 (74%)]\tlearningLoss: 0.008624\twhole_loss: 0.034005 \n",
      "Train Epoch: 28 [13984/15120 (92%)]\tlearningLoss: 0.003139\twhole_loss: 0.000001 \n",
      "\n",
      "Test set : Average loss:54.7211, Accuracy: 3628/3780(96%), F1-score: 0.9580095644880888\n",
      "\n",
      "Train Epoch: 29 [2784/15120 (18%)]\tlearningLoss: 0.000495\twhole_loss: 0.000070 \n",
      "Train Epoch: 29 [5584/15120 (37%)]\tlearningLoss: 0.033305\twhole_loss: 0.000074 \n",
      "Train Epoch: 29 [8384/15120 (55%)]\tlearningLoss: 0.005868\twhole_loss: 0.000012 \n",
      "Train Epoch: 29 [11184/15120 (74%)]\tlearningLoss: 0.009938\twhole_loss: 0.000004 \n",
      "Train Epoch: 29 [13984/15120 (92%)]\tlearningLoss: 0.002510\twhole_loss: 0.000009 \n",
      "\n",
      "Test set : Average loss:60.3522, Accuracy: 3632/3780(96%), F1-score: 0.9589426062491431\n",
      "\n",
      "Train Epoch: 30 [2784/15120 (18%)]\tlearningLoss: 0.021676\twhole_loss: 0.001995 \n",
      "Train Epoch: 30 [5584/15120 (37%)]\tlearningLoss: 0.004836\twhole_loss: 0.000125 \n",
      "Train Epoch: 30 [8384/15120 (55%)]\tlearningLoss: 0.003022\twhole_loss: 0.000063 \n",
      "Train Epoch: 30 [11184/15120 (74%)]\tlearningLoss: 0.019778\twhole_loss: 0.005189 \n",
      "Train Epoch: 30 [13984/15120 (92%)]\tlearningLoss: 0.015878\twhole_loss: 0.001008 \n",
      "\n",
      "Test set : Average loss:73.9326, Accuracy: 3608/3780(95%), F1-score: 0.9522040142276422\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "age_model.train(epochs, log_interval,a_lr_sche)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train Epoch: 1 [2784/15120 (18%)]\tlearningLoss: 1.597810\twhole_loss: 0.358551 \n",
      "Train Epoch: 1 [5584/15120 (37%)]\tlearningLoss: 0.500214\twhole_loss: 0.411356 \n",
      "Train Epoch: 1 [8384/15120 (55%)]\tlearningLoss: 0.461373\twhole_loss: 0.577858 \n",
      "Train Epoch: 1 [11184/15120 (74%)]\tlearningLoss: 0.427987\twhole_loss: 0.423450 \n",
      "Train Epoch: 1 [13984/15120 (92%)]\tlearningLoss: 0.376770\twhole_loss: 0.197144 \n",
      "ageCate Minimum :: 78.19459466263652\n",
      "\n",
      "Test set : Average loss:78.1946, Accuracy: 3304/3780(87%), F1-score: 0.7082520819145356\n",
      "\n",
      "Train Epoch: 2 [2784/15120 (18%)]\tlearningLoss: 0.365023\twhole_loss: 0.113317 \n",
      "Train Epoch: 2 [5584/15120 (37%)]\tlearningLoss: 0.328279\twhole_loss: 0.135725 \n",
      "Train Epoch: 2 [8384/15120 (55%)]\tlearningLoss: 0.323985\twhole_loss: 0.475490 \n",
      "Train Epoch: 2 [11184/15120 (74%)]\tlearningLoss: 0.273864\twhole_loss: 0.424163 \n",
      "Train Epoch: 2 [13984/15120 (92%)]\tlearningLoss: 0.304876\twhole_loss: 0.254563 \n",
      "ageCate Minimum :: 75.78346095979214\n",
      "\n",
      "Test set : Average loss:75.7835, Accuracy: 3301/3780(87%), F1-score: 0.7390965573520384\n",
      "\n",
      "Train Epoch: 3 [2784/15120 (18%)]\tlearningLoss: 0.243656\twhole_loss: 0.231157 \n",
      "Train Epoch: 3 [5584/15120 (37%)]\tlearningLoss: 0.263802\twhole_loss: 0.536062 \n",
      "Train Epoch: 3 [8384/15120 (55%)]\tlearningLoss: 0.248180\twhole_loss: 0.437142 \n",
      "Train Epoch: 3 [11184/15120 (74%)]\tlearningLoss: 0.245078\twhole_loss: 0.148127 \n",
      "Train Epoch: 3 [13984/15120 (92%)]\tlearningLoss: 0.219723\twhole_loss: 0.094245 \n",
      "\n",
      "Test set : Average loss:79.5687, Accuracy: 3306/3780(87%), F1-score: 0.7579986687953394\n",
      "\n",
      "Train Epoch: 4 [2784/15120 (18%)]\tlearningLoss: 0.213539\twhole_loss: 0.077968 \n",
      "Train Epoch: 4 [5584/15120 (37%)]\tlearningLoss: 0.183181\twhole_loss: 0.111570 \n",
      "Train Epoch: 4 [8384/15120 (55%)]\tlearningLoss: 0.210785\twhole_loss: 0.083409 \n",
      "Train Epoch: 4 [11184/15120 (74%)]\tlearningLoss: 0.221737\twhole_loss: 0.220248 \n",
      "Train Epoch: 4 [13984/15120 (92%)]\tlearningLoss: 0.188270\twhole_loss: 0.578519 \n",
      "\n",
      "Test set : Average loss:78.6673, Accuracy: 3351/3780(89%), F1-score: 0.731084601405677\n",
      "\n",
      "Train Epoch: 5 [2784/15120 (18%)]\tlearningLoss: 0.160871\twhole_loss: 0.265720 \n",
      "Train Epoch: 5 [5584/15120 (37%)]\tlearningLoss: 0.174266\twhole_loss: 0.111793 \n",
      "Train Epoch: 5 [8384/15120 (55%)]\tlearningLoss: 0.159783\twhole_loss: 0.153936 \n",
      "Train Epoch: 5 [11184/15120 (74%)]\tlearningLoss: 0.164187\twhole_loss: 0.241549 \n",
      "Train Epoch: 5 [13984/15120 (92%)]\tlearningLoss: 0.157528\twhole_loss: 0.057367 \n",
      "ageCate Minimum :: 75.08264352241531\n",
      "\n",
      "Test set : Average loss:75.0826, Accuracy: 3365/3780(89%), F1-score: 0.7697005803441899\n",
      "\n",
      "Train Epoch: 6 [2784/15120 (18%)]\tlearningLoss: 0.117856\twhole_loss: 0.047038 \n",
      "Train Epoch: 6 [5584/15120 (37%)]\tlearningLoss: 0.122824\twhole_loss: 0.125212 \n",
      "Train Epoch: 6 [8384/15120 (55%)]\tlearningLoss: 0.112956\twhole_loss: 0.022803 \n",
      "Train Epoch: 6 [11184/15120 (74%)]\tlearningLoss: 0.117838\twhole_loss: 0.087857 \n",
      "Train Epoch: 6 [13984/15120 (92%)]\tlearningLoss: 0.156771\twhole_loss: 0.534157 \n",
      "\n",
      "Test set : Average loss:92.8809, Accuracy: 3280/3780(87%), F1-score: 0.7677065643758306\n",
      "\n",
      "Train Epoch: 7 [2784/15120 (18%)]\tlearningLoss: 0.088334\twhole_loss: 0.076699 \n",
      "Train Epoch: 7 [5584/15120 (37%)]\tlearningLoss: 0.121694\twhole_loss: 0.250387 \n",
      "Train Epoch: 7 [8384/15120 (55%)]\tlearningLoss: 0.095360\twhole_loss: 0.069391 \n",
      "Train Epoch: 7 [11184/15120 (74%)]\tlearningLoss: 0.097181\twhole_loss: 0.100866 \n",
      "Train Epoch: 7 [13984/15120 (92%)]\tlearningLoss: 0.113592\twhole_loss: 0.022036 \n",
      "\n",
      "Test set : Average loss:91.7438, Accuracy: 3359/3780(89%), F1-score: 0.7729282297133121\n",
      "\n",
      "Train Epoch: 8 [2784/15120 (18%)]\tlearningLoss: 0.070039\twhole_loss: 0.040079 \n",
      "Train Epoch: 8 [5584/15120 (37%)]\tlearningLoss: 0.113303\twhole_loss: 0.098093 \n",
      "Train Epoch: 8 [8384/15120 (55%)]\tlearningLoss: 0.064502\twhole_loss: 0.026587 \n",
      "Train Epoch: 8 [11184/15120 (74%)]\tlearningLoss: 0.073565\twhole_loss: 0.275096 \n",
      "Train Epoch: 8 [13984/15120 (92%)]\tlearningLoss: 0.072178\twhole_loss: 0.176612 \n",
      "\n",
      "Test set : Average loss:97.9223, Accuracy: 3348/3780(89%), F1-score: 0.7622214297066225\n",
      "\n",
      "Train Epoch: 9 [2784/15120 (18%)]\tlearningLoss: 0.045793\twhole_loss: 0.026749 \n",
      "Train Epoch: 9 [5584/15120 (37%)]\tlearningLoss: 0.044123\twhole_loss: 0.078347 \n",
      "Train Epoch: 9 [8384/15120 (55%)]\tlearningLoss: 0.060373\twhole_loss: 0.055589 \n",
      "Train Epoch: 9 [11184/15120 (74%)]\tlearningLoss: 0.094377\twhole_loss: 0.020164 \n",
      "Train Epoch: 9 [13984/15120 (92%)]\tlearningLoss: 0.074411\twhole_loss: 0.073421 \n",
      "\n",
      "Test set : Average loss:123.2563, Accuracy: 3296/3780(87%), F1-score: 0.7460034654887595\n",
      "\n",
      "Train Epoch: 10 [2784/15120 (18%)]\tlearningLoss: 0.041998\twhole_loss: 0.010197 \n",
      "Train Epoch: 10 [5584/15120 (37%)]\tlearningLoss: 0.056013\twhole_loss: 0.009852 \n",
      "Train Epoch: 10 [8384/15120 (55%)]\tlearningLoss: 0.086611\twhole_loss: 0.183986 \n",
      "Train Epoch: 10 [11184/15120 (74%)]\tlearningLoss: 0.052763\twhole_loss: 0.250832 \n",
      "Train Epoch: 10 [13984/15120 (92%)]\tlearningLoss: 0.073864\twhole_loss: 0.146075 \n",
      "\n",
      "Test set : Average loss:116.2863, Accuracy: 3299/3780(87%), F1-score: 0.7636612575518691\n",
      "\n",
      "Train Epoch: 11 [2784/15120 (18%)]\tlearningLoss: 0.034984\twhole_loss: 0.003751 \n",
      "Train Epoch: 11 [5584/15120 (37%)]\tlearningLoss: 0.033627\twhole_loss: 0.033052 \n",
      "Train Epoch: 11 [8384/15120 (55%)]\tlearningLoss: 0.074224\twhole_loss: 0.006593 \n",
      "Train Epoch: 11 [11184/15120 (74%)]\tlearningLoss: 0.060000\twhole_loss: 0.048898 \n",
      "Train Epoch: 11 [13984/15120 (92%)]\tlearningLoss: 0.048388\twhole_loss: 0.439212 \n",
      "\n",
      "Test set : Average loss:110.7079, Accuracy: 3283/3780(87%), F1-score: 0.7563175932499057\n",
      "\n",
      "Train Epoch: 12 [2784/15120 (18%)]\tlearningLoss: 0.033179\twhole_loss: 0.001848 \n",
      "Train Epoch: 12 [5584/15120 (37%)]\tlearningLoss: 0.023317\twhole_loss: 0.001434 \n",
      "Train Epoch: 12 [8384/15120 (55%)]\tlearningLoss: 0.033788\twhole_loss: 0.001313 \n",
      "Train Epoch: 12 [11184/15120 (74%)]\tlearningLoss: 0.062150\twhole_loss: 0.177956 \n",
      "Train Epoch: 12 [13984/15120 (92%)]\tlearningLoss: 0.039347\twhole_loss: 0.048727 \n",
      "\n",
      "Test set : Average loss:147.3229, Accuracy: 3264/3780(86%), F1-score: 0.7536603216011538\n",
      "\n",
      "Train Epoch: 13 [2784/15120 (18%)]\tlearningLoss: 0.049880\twhole_loss: 0.027215 \n",
      "Train Epoch: 13 [5584/15120 (37%)]\tlearningLoss: 0.038975\twhole_loss: 0.000349 \n",
      "Train Epoch: 13 [8384/15120 (55%)]\tlearningLoss: 0.013380\twhole_loss: 0.004314 \n",
      "Train Epoch: 13 [11184/15120 (74%)]\tlearningLoss: 0.045789\twhole_loss: 0.004175 \n",
      "Train Epoch: 13 [13984/15120 (92%)]\tlearningLoss: 0.033726\twhole_loss: 0.003761 \n",
      "\n",
      "Test set : Average loss:128.4798, Accuracy: 3382/3780(89%), F1-score: 0.767007020907663\n",
      "\n",
      "Train Epoch: 14 [2784/15120 (18%)]\tlearningLoss: 0.051974\twhole_loss: 0.053247 \n",
      "Train Epoch: 14 [5584/15120 (37%)]\tlearningLoss: 0.031082\twhole_loss: 0.008823 \n",
      "Train Epoch: 14 [8384/15120 (55%)]\tlearningLoss: 0.046737\twhole_loss: 0.054787 \n",
      "Train Epoch: 14 [11184/15120 (74%)]\tlearningLoss: 0.081709\twhole_loss: 0.010432 \n",
      "Train Epoch: 14 [13984/15120 (92%)]\tlearningLoss: 0.031045\twhole_loss: 0.045603 \n",
      "\n",
      "Test set : Average loss:163.8153, Accuracy: 3260/3780(86%), F1-score: 0.7514353810858428\n",
      "\n",
      "Train Epoch: 15 [2784/15120 (18%)]\tlearningLoss: 0.013279\twhole_loss: 0.011314 \n",
      "Train Epoch: 15 [5584/15120 (37%)]\tlearningLoss: 0.018212\twhole_loss: 0.021069 \n",
      "Train Epoch: 15 [8384/15120 (55%)]\tlearningLoss: 0.012003\twhole_loss: 0.000071 \n",
      "Train Epoch: 15 [11184/15120 (74%)]\tlearningLoss: 0.040912\twhole_loss: 0.018383 \n",
      "Train Epoch: 15 [13984/15120 (92%)]\tlearningLoss: 0.061376\twhole_loss: 0.013562 \n",
      "\n",
      "Test set : Average loss:159.1769, Accuracy: 3356/3780(89%), F1-score: 0.7647653258512989\n",
      "\n",
      "Train Epoch: 16 [2784/15120 (18%)]\tlearningLoss: 0.017633\twhole_loss: 0.010463 \n",
      "Train Epoch: 16 [5584/15120 (37%)]\tlearningLoss: 0.012529\twhole_loss: 0.006260 \n",
      "Train Epoch: 16 [8384/15120 (55%)]\tlearningLoss: 0.021920\twhole_loss: 0.193338 \n",
      "Train Epoch: 16 [11184/15120 (74%)]\tlearningLoss: 0.038457\twhole_loss: 0.001307 \n",
      "Train Epoch: 16 [13984/15120 (92%)]\tlearningLoss: 0.128438\twhole_loss: 0.001040 \n",
      "\n",
      "Test set : Average loss:132.9334, Accuracy: 3271/3780(87%), F1-score: 0.7580724990523892\n",
      "\n",
      "Train Epoch: 17 [2784/15120 (18%)]\tlearningLoss: 0.012749\twhole_loss: 0.001101 \n",
      "Train Epoch: 17 [5584/15120 (37%)]\tlearningLoss: 0.010712\twhole_loss: 0.049390 \n",
      "Train Epoch: 17 [8384/15120 (55%)]\tlearningLoss: 0.017867\twhole_loss: 0.031502 \n",
      "Train Epoch: 17 [11184/15120 (74%)]\tlearningLoss: 0.013022\twhole_loss: 0.001169 \n",
      "Train Epoch: 17 [13984/15120 (92%)]\tlearningLoss: 0.049621\twhole_loss: 0.000797 \n",
      "\n",
      "Test set : Average loss:134.1530, Accuracy: 3357/3780(89%), F1-score: 0.7773483197903674\n",
      "\n",
      "Train Epoch: 18 [2784/15120 (18%)]\tlearningLoss: 0.050345\twhole_loss: 0.195692 \n",
      "Train Epoch: 18 [5584/15120 (37%)]\tlearningLoss: 0.058713\twhole_loss: 0.052911 \n",
      "Train Epoch: 18 [8384/15120 (55%)]\tlearningLoss: 0.019438\twhole_loss: 0.004005 \n",
      "Train Epoch: 18 [11184/15120 (74%)]\tlearningLoss: 0.012986\twhole_loss: 0.000300 \n",
      "Train Epoch: 18 [13984/15120 (92%)]\tlearningLoss: 0.029642\twhole_loss: 0.049757 \n",
      "\n",
      "Test set : Average loss:155.1739, Accuracy: 3331/3780(88%), F1-score: 0.7312418030404962\n",
      "\n",
      "Train Epoch: 19 [2784/15120 (18%)]\tlearningLoss: 0.015782\twhole_loss: 0.000111 \n",
      "Train Epoch: 19 [5584/15120 (37%)]\tlearningLoss: 0.011124\twhole_loss: 0.000790 \n",
      "Train Epoch: 19 [8384/15120 (55%)]\tlearningLoss: 0.027541\twhole_loss: 0.190925 \n",
      "Train Epoch: 19 [11184/15120 (74%)]\tlearningLoss: 0.021884\twhole_loss: 0.038487 \n",
      "Train Epoch: 19 [13984/15120 (92%)]\tlearningLoss: 0.028611\twhole_loss: 0.000852 \n",
      "\n",
      "Test set : Average loss:148.8222, Accuracy: 3250/3780(86%), F1-score: 0.7572485341511935\n",
      "\n",
      "Train Epoch: 20 [2784/15120 (18%)]\tlearningLoss: 0.042442\twhole_loss: 0.007525 \n",
      "Train Epoch: 20 [5584/15120 (37%)]\tlearningLoss: 0.055704\twhole_loss: 0.085919 \n",
      "Train Epoch: 20 [8384/15120 (55%)]\tlearningLoss: 0.019321\twhole_loss: 0.000277 \n",
      "Train Epoch: 20 [11184/15120 (74%)]\tlearningLoss: 0.035549\twhole_loss: 0.001127 \n",
      "Train Epoch: 20 [13984/15120 (92%)]\tlearningLoss: 0.021156\twhole_loss: 0.005022 \n",
      "\n",
      "Test set : Average loss:161.2128, Accuracy: 3342/3780(88%), F1-score: 0.7634197250436152\n",
      "\n",
      "Train Epoch: 21 [2784/15120 (18%)]\tlearningLoss: 0.015006\twhole_loss: 0.000252 \n",
      "Train Epoch: 21 [5584/15120 (37%)]\tlearningLoss: 0.014540\twhole_loss: 0.003488 \n",
      "Train Epoch: 21 [8384/15120 (55%)]\tlearningLoss: 0.039967\twhole_loss: 0.003394 \n",
      "Train Epoch: 21 [11184/15120 (74%)]\tlearningLoss: 0.029022\twhole_loss: 0.062803 \n",
      "Train Epoch: 21 [13984/15120 (92%)]\tlearningLoss: 0.040694\twhole_loss: 0.006725 \n",
      "\n",
      "Test set : Average loss:163.6816, Accuracy: 3385/3780(90%), F1-score: 0.7598141364766365\n",
      "\n",
      "Train Epoch: 22 [2784/15120 (18%)]\tlearningLoss: 0.011971\twhole_loss: 0.000256 \n",
      "Train Epoch: 22 [5584/15120 (37%)]\tlearningLoss: 0.026975\twhole_loss: 0.002793 \n",
      "Train Epoch: 22 [8384/15120 (55%)]\tlearningLoss: 0.036484\twhole_loss: 0.000980 \n",
      "Train Epoch: 22 [11184/15120 (74%)]\tlearningLoss: 0.017496\twhole_loss: 0.008331 \n",
      "Train Epoch: 22 [13984/15120 (92%)]\tlearningLoss: 0.051885\twhole_loss: 0.019850 \n",
      "\n",
      "Test set : Average loss:135.1158, Accuracy: 3389/3780(90%), F1-score: 0.7790169833290078\n",
      "\n",
      "Train Epoch: 23 [2784/15120 (18%)]\tlearningLoss: 0.012320\twhole_loss: 0.003163 \n",
      "Train Epoch: 23 [5584/15120 (37%)]\tlearningLoss: 0.024519\twhole_loss: 0.000718 \n",
      "Train Epoch: 23 [8384/15120 (55%)]\tlearningLoss: 0.021218\twhole_loss: 0.000090 \n",
      "Train Epoch: 23 [11184/15120 (74%)]\tlearningLoss: 0.020025\twhole_loss: 0.009609 \n",
      "Train Epoch: 23 [13984/15120 (92%)]\tlearningLoss: 0.005494\twhole_loss: 0.000109 \n",
      "\n",
      "Test set : Average loss:254.6547, Accuracy: 3197/3780(85%), F1-score: 0.739608492657441\n",
      "\n",
      "Train Epoch: 24 [2784/15120 (18%)]\tlearningLoss: 0.015452\twhole_loss: 0.000129 \n",
      "Train Epoch: 24 [5584/15120 (37%)]\tlearningLoss: 0.018731\twhole_loss: 0.001847 \n",
      "Train Epoch: 24 [8384/15120 (55%)]\tlearningLoss: 0.048041\twhole_loss: 0.073893 \n",
      "Train Epoch: 24 [11184/15120 (74%)]\tlearningLoss: 0.032531\twhole_loss: 0.000642 \n",
      "Train Epoch: 24 [13984/15120 (92%)]\tlearningLoss: 0.052838\twhole_loss: 0.026063 \n",
      "\n",
      "Test set : Average loss:148.0406, Accuracy: 3304/3780(87%), F1-score: 0.7753157659588038\n",
      "\n",
      "Train Epoch: 25 [2784/15120 (18%)]\tlearningLoss: 0.011980\twhole_loss: 0.000298 \n",
      "Train Epoch: 25 [5584/15120 (37%)]\tlearningLoss: 0.009270\twhole_loss: 0.012942 \n",
      "Train Epoch: 25 [8384/15120 (55%)]\tlearningLoss: 0.015350\twhole_loss: 0.001791 \n",
      "Train Epoch: 25 [11184/15120 (74%)]\tlearningLoss: 0.046510\twhole_loss: 0.000129 \n",
      "Train Epoch: 25 [13984/15120 (92%)]\tlearningLoss: 0.016629\twhole_loss: 0.001345 \n",
      "\n",
      "Test set : Average loss:151.4885, Accuracy: 3344/3780(88%), F1-score: 0.7824249295123461\n",
      "\n",
      "Train Epoch: 26 [2784/15120 (18%)]\tlearningLoss: 0.033878\twhole_loss: 0.001009 \n",
      "Train Epoch: 26 [5584/15120 (37%)]\tlearningLoss: 0.017797\twhole_loss: 0.054127 \n",
      "Train Epoch: 26 [8384/15120 (55%)]\tlearningLoss: 0.014387\twhole_loss: 0.027019 \n",
      "Train Epoch: 26 [11184/15120 (74%)]\tlearningLoss: 0.007900\twhole_loss: 0.000582 \n",
      "Train Epoch: 26 [13984/15120 (92%)]\tlearningLoss: 0.024124\twhole_loss: 0.000162 \n",
      "\n",
      "Test set : Average loss:166.1821, Accuracy: 3290/3780(87%), F1-score: 0.7583337646405627\n",
      "\n",
      "Train Epoch: 27 [2784/15120 (18%)]\tlearningLoss: 0.020216\twhole_loss: 0.000039 \n",
      "Train Epoch: 27 [5584/15120 (37%)]\tlearningLoss: 0.019561\twhole_loss: 0.000125 \n",
      "Train Epoch: 27 [8384/15120 (55%)]\tlearningLoss: 0.031984\twhole_loss: 0.091138 \n",
      "Train Epoch: 27 [11184/15120 (74%)]\tlearningLoss: 0.026602\twhole_loss: 0.000126 \n",
      "Train Epoch: 27 [13984/15120 (92%)]\tlearningLoss: 0.010889\twhole_loss: 0.003344 \n",
      "\n",
      "Test set : Average loss:191.6796, Accuracy: 3331/3780(88%), F1-score: 0.7607443756587596\n",
      "\n",
      "Train Epoch: 28 [2784/15120 (18%)]\tlearningLoss: 0.023270\twhole_loss: 0.001201 \n",
      "Train Epoch: 28 [5584/15120 (37%)]\tlearningLoss: 0.018410\twhole_loss: 0.000969 \n",
      "Train Epoch: 28 [8384/15120 (55%)]\tlearningLoss: 0.013775\twhole_loss: 0.001084 \n",
      "Train Epoch: 28 [11184/15120 (74%)]\tlearningLoss: 0.023939\twhole_loss: 0.002742 \n",
      "Train Epoch: 28 [13984/15120 (92%)]\tlearningLoss: 0.018992\twhole_loss: 0.015910 \n",
      "\n",
      "Test set : Average loss:202.6805, Accuracy: 3247/3780(86%), F1-score: 0.7317686275590819\n",
      "\n",
      "Train Epoch: 29 [2784/15120 (18%)]\tlearningLoss: 0.045568\twhole_loss: 0.000081 \n",
      "Train Epoch: 29 [5584/15120 (37%)]\tlearningLoss: 0.027053\twhole_loss: 0.001485 \n",
      "Train Epoch: 29 [8384/15120 (55%)]\tlearningLoss: 0.017245\twhole_loss: 0.003132 \n",
      "Train Epoch: 29 [11184/15120 (74%)]\tlearningLoss: 0.011295\twhole_loss: 0.020677 \n",
      "Train Epoch: 29 [13984/15120 (92%)]\tlearningLoss: 0.018456\twhole_loss: 0.001932 \n",
      "\n",
      "Test set : Average loss:177.8881, Accuracy: 3345/3780(88%), F1-score: 0.7755399938895976\n",
      "\n",
      "Train Epoch: 30 [2784/15120 (18%)]\tlearningLoss: 0.006097\twhole_loss: 0.000019 \n",
      "Train Epoch: 30 [5584/15120 (37%)]\tlearningLoss: 0.001597\twhole_loss: 0.000009 \n",
      "Train Epoch: 30 [8384/15120 (55%)]\tlearningLoss: 0.044629\twhole_loss: 0.026404 \n",
      "Train Epoch: 30 [11184/15120 (74%)]\tlearningLoss: 0.041668\twhole_loss: 0.081349 \n",
      "Train Epoch: 30 [13984/15120 (92%)]\tlearningLoss: 0.029096\twhole_loss: 0.000308 \n",
      "\n",
      "Test set : Average loss:161.8151, Accuracy: 3364/3780(89%), F1-score: 0.7659035541128958\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "PATH = './weights/'\n",
    "m_checkpoint = torch.load(PATH + 'mask_all.tar') \n",
    "\n",
    "mask_model.model.load_state_dict(m_checkpoint['model'])\n",
    "mask_model.optimizer.load_state_dict(m_checkpoint['optimizer'])\n",
    "\n",
    "g_checkpoint = torch.load(PATH + 'gender_all.tar') \n",
    "\n",
    "gender_model.model.load_state_dict(g_checkpoint['model'])\n",
    "gender_model.optimizer.load_state_dict(g_checkpoint['optimizer'])\n",
    "\n",
    "a_checkpoint = torch.load(PATH + 'ageCate_all.tar') \n",
    "\n",
    "age_model.model.load_state_dict(a_checkpoint['model'])\n",
    "age_model.optimizer.load_state_dict(a_checkpoint['optimizer'])\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "with torch.no_grad():\n",
    "    mask_model.model.eval()\n",
    "    gender_model.model.eval()\n",
    "    age_model.model.eval()\n",
    "    \n",
    "    sub_list = []\n",
    "    mask_list = []\n",
    "    gender_list = []\n",
    "    age_list = []\n",
    "\n",
    "    for eval_x in eval_loader:\n",
    "        outputs1 = mask_model.model(eval_x.cuda())\n",
    "        outputs2 = gender_model.model(eval_x.cuda())\n",
    "        outputs3 = age_model.model(eval_x.cuda())\n",
    "\n",
    "        _, predicted1 = torch.max(outputs1.data, 1)\n",
    "        _, predicted2 = torch.max(outputs2.data, 1)\n",
    "        _, predicted3 = torch.max(outputs3.data, 1)\n",
    "\n",
    "        sub_list.append([int(predicted1),int(predicted2),int(predicted3)])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "len(sub_list)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "12600"
      ]
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "classes_dict = {\n",
    "        'Wear' : 0, 'NotWear' : 1, 'Incorrect' : 2,\n",
    "        \"female\" : 1, \"male\" :0,\n",
    "        'upper60' : 2 , \"30to60\":1, \"lower30\" :0,\n",
    "        \n",
    "    }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "str(sub_list[0])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'[0, 0, 0]'"
      ]
     },
     "metadata": {},
     "execution_count": 75
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "answer_dict = {\n",
    "    '[0, 0, 0]' : 0, '[0, 0, 1]' : 1,\n",
    "    '[0, 0, 2]' : 2, '[0, 1, 0]':3, '[0, 1, 1]':4, '[0, 1, 2]':5,\n",
    "    '[2, 0, 0]':6, '[2, 0, 1]' :7, '[2, 0, 2]':8, '[2, 1, 0]':9, '[2, 1, 1]':10,\n",
    "    '[2, 1, 2]':11, '[1, 0, 0]':12, '[1, 0, 1]':13, '[1, 0, 2]':14, '[1, 1, 0]':15, '[1, 1, 1]': 16,\n",
    "    '[1, 1, 2]':17\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "submission = pd.read_csv('/opt/ml/input/data/eval/info.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "source": [
    "len(submission)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "12600"
      ]
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "len(sub_list)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "12600"
      ]
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "source": [
    "ans = [answer_dict[str(list_ans)] for list_ans in sub_list]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "source": [
    "sub_dict = {key:val for key, val in zip(eval_set.get_id_list(), ans)}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "final_ans = [sub_dict[id_img] for id_img in submission['ImageID'].values.tolist()]\n",
    "submission['ans'] = final_ans"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "source": [
    "submission.to_csv('./submission.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "source": [
    "submission"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                            ImageID  ans\n",
       "0      cbc5c6e168e63498590db46022617123f1fe1268.jpg   13\n",
       "1      0e72482bf56b3581c081f7da2a6180b8792c7089.jpg   13\n",
       "2      b549040c49190cedc41327748aeb197c1670f14d.jpg   13\n",
       "3      4f9cb2a045c6d5b9e50ad3459ea7b791eb6e18bc.jpg   13\n",
       "4      248428d9a4a5b6229a7081c32851b90cb8d38d0c.jpg    0\n",
       "...                                             ...  ...\n",
       "12595  d71d4570505d6af8f777690e63edfa8d85ea4476.jpg    0\n",
       "12596  6cf1300e8e218716728d5820c0bab553306c2cfd.jpg   16\n",
       "12597  8140edbba31c3a824e817e6d5fb95343199e2387.jpg    3\n",
       "12598  030d439efe6fb5a7bafda45a393fc19f2bf57f54.jpg   13\n",
       "12599  f1e0b9594ae9f72571f0a9dc67406ad41f2edab0.jpg   13\n",
       "\n",
       "[12600 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageID</th>\n",
       "      <th>ans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cbc5c6e168e63498590db46022617123f1fe1268.jpg</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0e72482bf56b3581c081f7da2a6180b8792c7089.jpg</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b549040c49190cedc41327748aeb197c1670f14d.jpg</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4f9cb2a045c6d5b9e50ad3459ea7b791eb6e18bc.jpg</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>248428d9a4a5b6229a7081c32851b90cb8d38d0c.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12595</th>\n",
       "      <td>d71d4570505d6af8f777690e63edfa8d85ea4476.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12596</th>\n",
       "      <td>6cf1300e8e218716728d5820c0bab553306c2cfd.jpg</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12597</th>\n",
       "      <td>8140edbba31c3a824e817e6d5fb95343199e2387.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12598</th>\n",
       "      <td>030d439efe6fb5a7bafda45a393fc19f2bf57f54.jpg</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12599</th>\n",
       "      <td>f1e0b9594ae9f72571f0a9dc67406ad41f2edab0.jpg</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12600 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 87
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}