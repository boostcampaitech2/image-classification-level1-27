{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b5ff916d-beaa-40d6-886f-5ecfb32c2c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from importlib import import_module\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from tqdm import tqdm\n",
    "from transform import get_tta_transform\n",
    "from PIL import Image\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934ecc8e-652d-4993-ab28-7655ae4392b5",
   "metadata": {},
   "source": [
    "## valid dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "88ac8ef2-d9b1-43ce-8cf3-f3c2dc39a4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        super(CustomDataset, self).__init__()\n",
    "        self.data_dir = \"/opt/ml/input/data/train/crop_images/\"\n",
    "        self.info_path = \"/opt/ml/input/data/train/train.csv\"\n",
    "        self.k = 5\n",
    "        self.seed = 1997\n",
    "        self.k_index= 4\n",
    "        self.train = False\n",
    "\n",
    "        self.folders = None\n",
    "        self.set_k_fold(self.info_path, k_index=self.k_index, k=self.k, seed=self.seed)\n",
    "\n",
    "        self.input_files = []\n",
    "        self.images = []\n",
    "        self.masks = []\n",
    "        self.genders = []\n",
    "        self.ages = []\n",
    "        self.labels = []\n",
    "        \n",
    "\n",
    "        self.num_classes = None\n",
    "\n",
    "        ### prepare images and labels\n",
    "        if self.train:\n",
    "            print(\"Train Data Loading...\")\n",
    "            age_count = [0,0,0]\n",
    "        else:\n",
    "            print(\"Test Data Loading...\")\n",
    "\n",
    "        for directory in tqdm(self.folders):\n",
    "            image_dir = os.path.join(self.data_dir, directory)\n",
    "            ID, GENDER, RACE, real_AGE = directory.split('_')\n",
    "\n",
    "            if GENDER == \"male\":\n",
    "                GENDER = 0\n",
    "            elif GENDER ==\"female\":\n",
    "                GENDER = 1\n",
    "\n",
    "            # fix gender label error\n",
    "            if self.train:\n",
    "                if GENDER == \"male\":\n",
    "                    GENDER = 0.95\n",
    "                elif GENDER ==\"female\":\n",
    "                    GENDER = 0.05\n",
    "\n",
    "                if ID in ['006359', '006360', '006361', '006362', '006363', '006364', '001498-1', '004432']:\n",
    "                    GENDER = 0.05 if GENDER==0.95 else 0.95\n",
    "                elif ID in ['003724', '003421', '003399', '001200', '005223', '001270', '006226', '000664']:\n",
    "                    GENDER = 0.5\n",
    "            \n",
    "            else:\n",
    "                if GENDER == \"male\":\n",
    "                    GENDER = 1\n",
    "                elif GENDER ==\"female\":\n",
    "                    GENDER = 0\n",
    "                if ID in ['006359', '006360', '006361', '006362', '006363', '006364', '001498-1', '004432']:\n",
    "                    GENDER = 0 if GENDER==1 else 1\n",
    "\n",
    "            if self.train:\n",
    "                if int(real_AGE) < 30:\n",
    "                    AGE = 0\n",
    "                    age_count[0] += 1\n",
    "                elif int(real_AGE) < 60:\n",
    "                    AGE = 1\n",
    "                    age_count[1] += 1\n",
    "                else:\n",
    "                    AGE = 2\n",
    "                    age_count[2] += 1\n",
    "            else:\n",
    "                if int(real_AGE) < 30:\n",
    "                    AGE = 0\n",
    "                elif int(real_AGE) < 60:\n",
    "                    AGE = 1\n",
    "                else:\n",
    "                    AGE = 2\n",
    "        \n",
    "        \n",
    "            file_list = [f for f in os.listdir(image_dir) if f[0] != '.']\n",
    "            for file in file_list:\n",
    "                self.input_files.append(os.path.join(image_dir, file))\n",
    "                \n",
    "                if file[0:4] == \"mask\":\n",
    "                    MASK = 0\n",
    "                elif file[0:9] == \"incorrect\":\n",
    "                    MASK = 1\n",
    "                elif file[0:6] == \"normal\":\n",
    "                    MASK = 2\n",
    "                else:\n",
    "                    raise\n",
    "\n",
    "                self.images.append(np.array(Image.open(os.path.join(image_dir, file))))    \n",
    "                self.masks.append(MASK)\n",
    "                self.genders.append(GENDER)\n",
    "                self.ages.append(AGE)\n",
    "                self.labels.append(MASK*6 + GENDER*3 + AGE)\n",
    "\n",
    "        if self.train:\n",
    "            self.age_weight = [1/i for i in age_count]\n",
    "            self.class_weight = [self.age_weight[a] for a in self.ages]\n",
    "                    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ### load image\n",
    "        image = self.images[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image=image)['image']\n",
    "        image.type(torch.float32)\n",
    "        \n",
    "        ### load label\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "    \n",
    "        \n",
    "        return image, label\n",
    "\n",
    "    def set_k_fold(self, info_path, k_index=None, k=5, seed=1997):\n",
    "        \"\"\"\n",
    "            output: train_folder, valid_folder\n",
    "        \"\"\"\n",
    "\n",
    "        if not k_index in range(k):\n",
    "            raise Exception('n_splits에 맞는 index를 입력해주세요')\n",
    "\n",
    "        train_info = pd.read_csv(info_path)\n",
    "\n",
    "        ### age/gender 동일 비율로 K Fold진행\n",
    "        new_age = np.array(train_info['age'])\n",
    "        new_gender = np.array(train_info['gender'])\n",
    "        str_for_split = new_age.astype(str)+new_gender\n",
    "\n",
    "        SFK = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "        for idx, (train_index, valid_index) in enumerate(SFK.split(train_info, str_for_split)):\n",
    "            if idx == k_index:\n",
    "                self.folders = train_info['path'][train_index] if self.train else train_info['path'][valid_index]\n",
    "                break\n",
    "\n",
    "    def set_transform(self, transform):\n",
    "        self.transform = transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "46bfcc49-12dd-4124-8b3a-6b945e5d213c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_module = getattr(import_module(\"model\"), \"CustomModel\")  # default: BaseModel\n",
    "\n",
    "model = model_module().to(device)\n",
    "model.load_state_dict(torch.load(\"./model/ensemble4/best_acc.pth\", map_location=device))\n",
    "model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a120f3ff-1dc0-411a-a703-490fdbdc56c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/model_selection/_split.py:666: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n",
      "  1%|▏         | 7/540 [00:00<00:08, 63.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data Loading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 540/540 [00:10<00:00, 53.90it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_module = getattr(import_module(\"dataset\"), 'CustomDataset')  # default: BaseAugmentation\n",
    "val_set = CustomDataset()\n",
    "\n",
    "transform_module = getattr(import_module(\"transform\"), 'Augmentation_384')\n",
    "val_transform = transform_module(\n",
    "        train=False\n",
    "    )\n",
    "val_set.set_transform(val_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "64aeb75f-3a64-476f-86ec-764d1885a059",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = DataLoader(\n",
    "        val_set,\n",
    "        batch_size=2,\n",
    "        num_workers=2,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d477d551-e90c-4d88-8fac-b3ba16f3e4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tta(tta_transforms, model, inputs):\n",
    "    m_out_list = [[],[],[]]\n",
    "    g_out_list = []\n",
    "    a_out_list = [[],[],[]]\n",
    "    for transformer in tta_transforms: # custom transforms or e.g. tta.aliases.d4_transform() \n",
    "                    # augment image\n",
    "        augmented_image = transformer.augment_image(inputs)\n",
    "        m, g, a = model(augmented_image)\n",
    "                    \n",
    "        m_tensor = nn.functional.softmax(m).cpu()\n",
    "        g_tensor = torch.sigmoid(g).cpu()\n",
    "        a_tensor = nn.functional.softmax(a).cpu()\n",
    "                    # save results\n",
    "        m_out_list[0].append(m_tensor[:,0])\n",
    "        m_out_list[1].append(m_tensor[:,1])\n",
    "        m_out_list[2].append(m_tensor[:,2])\n",
    "        g_out_list.append(g_tensor)\n",
    "        a_out_list[0].append(a_tensor[:,0])\n",
    "        a_out_list[1].append(a_tensor[:,1])\n",
    "        a_out_list[2].append(a_tensor[:,2])\n",
    "                    \n",
    "            # reduce results as you want, e.g mean/max/min\n",
    "    m_out_list[0] = torch.tensor(m_out_list)\n",
    "    print(a)\n",
    "    raise\n",
    "    m1_result = torch.mean(torch.tensor(m_out_list[0]),1)\n",
    "    m2_result = torch.mean(torch.tensor(m_out_list[1]),1)\n",
    "    m3_result = torch.mean(torch.tensor(m_out_list[2]),1)\n",
    "    a1_result = torch.mean(torch.tensor(a_out_list[0]),1)\n",
    "    a2_result = torch.mean(torch.tensor(a_out_list[1]),1)\n",
    "    a3_result = torch.mean(torch.tensor(a_out_list[2]),1)\n",
    "    g_result = torch.mean(torch.tensor(g_out_list),1)\n",
    "\n",
    "    m_outs = torch.tensor([m1_result, m2_result, m3_result])\n",
    "    a_outs = torch.tensor([a1_result, a2_result, a3_result])\n",
    "\n",
    "    return m_outs, torch.tensor(g_result).cpu(), a_outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "533407ff-d831-4e42-a899-a505e650cb65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1890 [00:00<?, ?it/s]<ipython-input-118-82b0c8a9c73f>:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  m_tensor = nn.functional.softmax(m).cpu()\n",
      "<ipython-input-118-82b0c8a9c73f>:12: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  a_tensor = nn.functional.softmax(a).cpu()\n",
      "  0%|          | 0/1890 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-119-79c7ba01d9a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mm_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtta_transforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mtta_m_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-118-82b0c8a9c73f>\u001b[0m in \u001b[0;36mtta\u001b[0;34m(tta_transforms, model, inputs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;31m# reduce results as you want, e.g mean/max/min\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm_out_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "ans = []\n",
    "target = []\n",
    "with torch.no_grad():\n",
    "    for idx, (images, labels) in enumerate(tqdm(val_loader)):\n",
    "        images = images.to(device)\n",
    "\n",
    "\n",
    "        m_outs, g_outs, a_outs = tta(tta_transforms, model, images)\n",
    "\n",
    "        tta_m_preds = torch.unsqueeze(torch.argmax(m_outs, dim=-1),0).cpu()\n",
    "        tta_a_preds = torch.unsqueeze(torch.argmax(a_outs, dim=-1),0).cpu()\n",
    "        if g_outs >= 0.5 : tta_g_preds = 1\n",
    "        else: tta_g_preds= 0\n",
    "        tta_g_preds = torch.unsqueeze(torch.tensor(tta_g_preds),0).cpu()\n",
    "        tta_preds = label_encoder(tta_m_preds, tta_g_preds, tta_a_preds)\n",
    "        ans.append(tta_preds[0].item())\n",
    "        \n",
    "        target.append(labels)\n",
    "    target = torch.cat(target)\n",
    "    ans = torch.tensor(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b0059c00-73a9-47e3-83fa-f53d34642c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/numpy/lib/function_base.py:380: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "/opt/conda/lib/python3.8/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(target,ans,average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9278688-0ddc-44b8-b8cf-85aca1ad5067",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
